################################################
# A-TEAM ë²•ë¥  RAG ì±—ë´‡ (LangGraph V8)
# V8 ë¦¬íŒ©í† ë§:
# - @dataclass Configë¡œ ì„¤ì • ë¶„ë¦¬
# - ê³„ì¸µí™”ëœ êµ¬ì¡°: Infrastructure â†’ Logic â†’ Execution
# - ì½”ë“œ ê°€ë…ì„± í–¥ìƒ
# ê¸°ì¡´ ê¸°ëŠ¥: ì§ˆë¬¸ ì˜ë„ ë¶„ì„, Hybrid Retriever, Query Expansion, Generator-Critic
# ì‘ì„±ì: SKN 3-1íŒ€ A-TEAM
# ì‘ì„±ì¼: 2026-01-08
################################################


import os
import sys
import logging
import warnings
import asyncio
from pathlib import Path
from dataclasses import dataclass, field
from typing import (
    Annotated, TypedDict, Sequence, Optional, List, Literal, Dict, Any
)

# Third-party
import torch
from dotenv import load_dotenv, find_dotenv
from pydantic import BaseModel, Field
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# LangChain Core
from langchain_core.documents import Document, BaseDocumentCompressor
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings


# Qdrant & FlagEmbedding (BGE-M3)
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient, AsyncQdrantClient, models
from FlagEmbedding import BGEM3FlagModel

# LangGraph
from langgraph.graph.message import add_messages
from langgraph.graph import StateGraph, END
from langgraph.graph.state import CompiledStateGraph
from langchain_core.runnables import RunnableLambda

# LangSmith Tracing
from langsmith import traceable


# Load Environment Variables
load_dotenv(find_dotenv())


# ============================================================
# [SECTION 1] Configuration - ëª¨ë“  ì„¤ì •ê°’ì„ í•œ ê³³ì—ì„œ ê´€ë¦¬
# ============================================================
@dataclass
class Config:
    """Application Configuration (dataclass)

    ëª¨ë“  í•˜ë“œì½”ë”©ëœ ê°’ì„ ì´ê³³ì—ì„œ ê´€ë¦¬í•©ë‹ˆë‹¤.
    ë³€ê²½ì´ í•„ìš”í•œ ê²½ìš° ì´ í´ë˜ìŠ¤ë§Œ ìˆ˜ì •í•˜ë©´ ë©ë‹ˆë‹¤.
    """

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # [1] Models - ì‚¬ìš©í•  ëª¨ë¸ ì„¤ì •
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    LLM_MODEL: str = "gpt-4o-mini"
    LLM_TEMPERATURE: float = 0.0
    EMBEDDING_MODEL: str = "Qwen/Qwen3-Embedding-0.6B"
    SPARSE_EMBEDDING_MODEL: str = "BAAI/bge-m3"  # BGE-M3 (Multilingual)
    RERANKER_MODEL: str = "jinaai/jina-reranker-v2-base-multilingual"

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # [2] RAG Settings - ê²€ìƒ‰ ë° ì²˜ë¦¬ ì„¤ì •
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    VECTOR_DIM: int = 1024
    TOP_K_VECTOR: int = 10                  # Vector Search k (20 â†’ 10 ìµœì í™”)
    TOP_K_RERANK: int = 5                   # Reranker í›„ ìƒìœ„ kê°œ
    TOP_K_FINAL: int = 3                    # ìµœì¢… ë‹µë³€ ìƒì„±ì— ì‚¬ìš©í•  ë¬¸ì„œ ìˆ˜ (5 â†’ 3 ìµœì í™”)
    RELEVANCE_THRESHOLD: float = 0.2        # ìœ ì‚¬ë„ ì„ê³„ê°’
    MAX_RETRY: int = 2                      # ì¬ê²€ìƒ‰ ìµœëŒ€ íšŸìˆ˜

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # [3] Qdrant - ë²¡í„° DB ì„¤ì •
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    QDRANT_TIMEOUT: int = 10                # 30 â†’ 10ì´ˆë¡œ ìµœì í™”
    QDRANT_PREFER_GRPC: bool = True         # gRPC ì‚¬ìš© (ë” ë¹ ë¦„)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # [4] PROMPTS - ë…¸ë“œë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # --- [ë…¸ë“œ: Query Expansion] HyDE + Hybrid Search ìš© í”„ë¡¬í”„íŠ¸ ---
    PROMPT_QUERY_EXPANSION: str = """ë‹¹ì‹ ì€ ë²•ë¥  ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ì„ë¬´
ì‚¬ìš©ìì˜ ëª¨í˜¸í•œ ì§ˆë¬¸ì„ ê²€ìƒ‰ ì—”ì§„(Qdrant Hybrid Search)ì´ ì´í•´í•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜í•˜ì„¸ìš”.

## ì „ëµ
1. **í‚¤ì›Œë“œ ì¶”ì¶œ (Sparseìš©)**: ì¡°ì‚¬ ë“±ì„ ì œê±°í•œ í•µì‹¬ ë²•ë¥  ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ì„¸ìš”. (í‚¤ì›Œë“œ ë§¤ì¹­ ì¤‘ìš”)
   - ì˜ˆ: "í‡´ì§ê¸ˆ ëª» ë°›ì•˜ì–´ìš”" â†’ "ê·¼ë¡œê¸°ì¤€ë²• í‡´ì§ê¸ˆ ì§€ê¸‰ ì²­êµ¬"
2. **ì˜ë¯¸ ì¿¼ë¦¬ (Denseìš©)**: ì§ˆë¬¸ì˜ ì˜ë„ì™€ ë¬¸ë§¥ì„ í¬í•¨í•œ ìì—°ì–´ ë¬¸ì¥ì„ ì‘ì„±í•˜ì„¸ìš”.
   - ì˜ˆ: "í‡´ì§ê¸ˆ ì§€ê¸‰ ê¸°í•œê³¼ ì²­êµ¬ ë°©ë²•ì— ëŒ€í•œ ê·¼ë¡œê¸°ì¤€ë²• ê·œì •"
3. **HyDE(ê°€ìƒ ë¬¸ì„œ)**: ì§ˆë¬¸ì— ëŒ€í•œ ì˜ˆìƒ ë‹µë³€ì„ 2ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
   - ì˜ˆ: "ê·¼ë¡œê¸°ì¤€ë²• ì œ36ì¡°ì— ë”°ë¥´ë©´ í‡´ì§ê¸ˆì€ í‡´ì§ í›„ 14ì¼ ì´ë‚´ì— ì§€ê¸‰í•´ì•¼ í•©ë‹ˆë‹¤..."

## ì¶œë ¥ ê·œì¹™
- keyword_query: Sparse ê²€ìƒ‰ìš© (ì¡°ì‚¬ ì œê±°, í•µì‹¬ ëª…ì‚¬ë§Œ, 50ì ì´ë‚´)
- semantic_query: Dense ê²€ìƒ‰ìš© (ì˜ë„ í¬í•¨ ìì—°ì–´, 100ì ì´ë‚´)  
- hyde_passage: Dense ê²€ìƒ‰ìš© ê°€ìƒ ë¬¸ì„œ (2ë¬¸ì¥, ë²•ë ¹ëª…/ì¡°í•­ í¬í•¨)"""

    # --- [ë…¸ë“œ: Analyze] ì§ˆë¬¸ ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ---
    PROMPT_ANALYZE: str = """ë‹¹ì‹ ì€ ë²•ë¥  ì§ˆë¬¸ì„ ì‹¬ì¸µ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ë¶„ë¥˜
- category: ë…¸ë™ë²•, í˜•ì‚¬ë²•, ë¯¼ì‚¬ë²•, ê¸°íƒ€
- intent_type: ë²•ë ¹ì¡°íšŒ, ì ˆì°¨ë¬¸ì˜, ìƒí™©íŒë‹¨, ê¶Œë¦¬í™•ì¸, ë¶„ìŸí•´ê²°, ì¼ë°˜ìƒë‹´
- query_complexity: ì§ˆë¬¸ì˜ ë‚œì´ë„ í‰ê°€
  * simple: ë‹¨ìˆœ ë²•ë ¹ ì¡°íšŒ, ì •ì˜ í™•ì¸ (ì˜ˆ: "ê·¼ë¡œê¸°ì¤€ë²• ì œ2ì¡°ê°€ ë­ì•¼?")
  * medium: ì¼ë°˜ì ì¸ ìƒí™© íŒë‹¨, ì ˆì°¨ ë¬¸ì˜
  * complex: ë³µì¡í•œ ë²•ì  í•´ì„, ì—¬ëŸ¬ ë²•ë ¹ ë¹„êµ, íŒë¡€ í•„ìš”
- search_strategy: ë²•ë ¹ìš°ì„ , í–‰ì •í•´ì„ìš°ì„ , íŒë¡€í•„ìˆ˜, ì¢…í•©ê²€ìƒ‰
- target_doc_types: ë²•, ì‹œí–‰ë ¹, ì‹œí–‰ê·œì¹™, í–‰ì •í•´ì„, íŒì •ì„ ë¡€

## ê·œì¹™
- needs_clarification: 1~2ë‹¨ì–´ë§Œ ìˆì–´ ë‹µë³€ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ì—ë§Œ true
- needs_case_law: íŒë¡€ ì–¸ê¸‰ ë˜ëŠ” ë²•ì  í•´ì„ ìŸì ì´ ìˆëŠ” ê²½ìš° true"""

    # --- [ë…¸ë“œ: Generate] Chain of Thought + In-Context Citation í”„ë¡¬í”„íŠ¸ ---
    PROMPT_GENERATE: str = """ë‹¹ì‹ ì€ ì—„ê²©í•œ ê¸°ì¤€ì„ ê°€ì§„ ë²•ë¥  AI 'A-TEAM'ì…ë‹ˆë‹¤.

## í•µì‹¬ ì›ì¹™
1. **ì¦ê±° ê¸°ë°˜**: ë°˜ë“œì‹œ ì œê³µëœ [ê²€ìƒ‰ëœ ë¬¸ì„œ]ì— ìˆëŠ” ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
2. **Hallucination ê¸ˆì§€**: ë¬¸ì„œì— ì—†ëŠ” ë²•ì¡°ë¬¸, íŒë¡€, ì‚¬ì‹¤ì„ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
3. **ì—„ê²©í•œ ì¸ìš©**: ëª¨ë“  ì‚¬ì‹¤ì  ì§„ìˆ  ë’¤ì— ë°˜ë“œì‹œ ì¶œì²˜ ì¸ë±ìŠ¤ë¥¼ í‘œê¸°í•˜ì„¸ìš”. (ì˜ˆ: ...ì§€ê¸‰í•´ì•¼ í•©ë‹ˆë‹¤[1].)

## ë‹µë³€ í˜•ì‹ (ë°˜ë“œì‹œ ì´ êµ¬ì¡°ë¡œ ì‘ì„±)

**ğŸ¤” ë¶„ì„**
(ì§ˆë¬¸ì˜ ë²•ì  ìŸì ê³¼ ì ìš© ê°€ëŠ¥í•œ ë²•ì¡°í•­ì„ ë¶„ì„í•˜ì„¸ìš”. ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì§ˆë¬¸ ê°„ì˜ ì—°ê²°ê³ ë¦¬ë¥¼ ì„œìˆ í•©ë‹ˆë‹¤.)

**ğŸ“Œ ê²°ë¡ **
(í•µì‹¬ ë‹µë³€ì„ 1-2ë¬¸ì¥ìœ¼ë¡œ ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”. ë°˜ë“œì‹œ ì¶œì²˜ ë²ˆí˜¸ë¥¼ ë¶™ì´ì„¸ìš”[1].)

**ğŸ“– ë²•ì  ê·¼ê±°**
- [ë²•ë ¹ëª… ì œXì¡°]: í•´ë‹¹ ì¡°í•­ ë‚´ìš© ìš”ì•½ [1]
- [ê´€ë ¨ ê·œì •]: ì¶”ê°€ ê·¼ê±° ìš”ì•½ [2]

**ğŸ’¡ ìœ ì˜ ì‚¬í•­**
(í•´ì„ìƒ ì£¼ì˜ì , ì˜ˆì™¸ ìƒí™©, ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•œ ì‚¬í•­ì„ ì•ˆë‚´í•˜ì„¸ìš”.)

## ì¸ìš© ê·œì¹™
- ê²€ìƒ‰ëœ ë¬¸ì„œëŠ” [ë¬¸ì„œ 1], [ë¬¸ì„œ 2], ... í˜•íƒœë¡œ ì œê³µë©ë‹ˆë‹¤.
- ë‹µë³€ì—ì„œ í•´ë‹¹ ë¬¸ì„œë¥¼ ì¸ìš©í•  ë•ŒëŠ” [1], [2], ... ë¡œ í‘œê¸°í•˜ì„¸ìš”.
- ë¬¸ì„œì— ì •ë³´ê°€ ì—†ìœ¼ë©´ "ì œê³µëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.

## ì–¸ì–´
- í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
- ë²•ë¥  ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•˜ì„¸ìš”."""

    # --- [ë…¸ë“œ: Evaluate] ë‹µë³€ í‰ê°€ìš© í”„ë¡¬í”„íŠ¸ ---
    PROMPT_EVALUATE: str = """ë‹¹ì‹ ì€ ë²•ë¥  ë‹µë³€ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ë¹„í‰ê°€ì…ë‹ˆë‹¤.

## í‰ê°€ ê¸°ì¤€
1. has_legal_basis: ë²•ë ¹ëª…, ì¡°í•­ ë²ˆí˜¸ ë“± êµ¬ì²´ì  ë²•ì  ê·¼ê±° ìˆëŠ”ê°€
2. cites_retrieved_docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì´ ë°˜ì˜ë˜ì—ˆëŠ”ê°€
3. is_relevant: ì§ˆë¬¸ì— ì§ì ‘ ë‹µí•˜ëŠ”ê°€
4. needs_more_search: ê²€ìƒ‰ ê²°ê³¼ ë¶€ì¡±í•˜ì—¬ ì¶”ê°€ ê²€ìƒ‰ í•„ìš”í•œê°€
5. quality_score: 1-5ì 

## ì›ì¹™
- í’ˆì§ˆ 3ì  ì´ìƒì´ë©´ í†µê³¼, 2ì  ì´í•˜ë©´ ì¬ê²€ìƒ‰ ê¶Œì¥"""

    # --- [ë…¸ë“œ: Clarify] ëª…í™•í™” ìš”ì²­ í…œí”Œë¦¿ ---
    TEMPLATE_CLARIFY: str = """ì•ˆë…•í•˜ì„¸ìš”! ì§ˆë¬¸ì„ ì˜ ì´í•´í•˜ê¸° ìœ„í•´ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.

{clarification_question}

ìœ„ ë‚´ìš©ì„ í¬í•¨í•´ì„œ ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì‹œë©´, ë” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ˜Š"""

    # --- [ë…¸ë“œ: Generate] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ ì‹œ ë‹µë³€ ---
    TEMPLATE_NO_RESULTS: str = """ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë²•ë¥  ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.

ë‹¤ìŒê³¼ ê°™ì´ ì‹œë„í•´ ë³´ì„¸ìš”:
1. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±
2. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì§ˆë¬¸
3. ì „ë¬¸ ë²•ë¥  ìƒë‹´ ê¶Œì¥

ğŸ“Œ ì°¸ê³ : https://law.go.kr"""


# ============================================================
# [SECTION 2] Logging Setup
# ============================================================
logging.basicConfig(
    format="%(asctime)s [%(levelname)s] %(name)s - %(message)s",
    level=logging.INFO,
    handlers=[logging.StreamHandler(sys.stdout)],
)
logger = logging.getLogger("LegalRAG-V8")


# ============================================================
# [SECTION 3] State Definition - LangGraph ìƒíƒœ ì •ì˜
# ============================================================
class AgentState(TypedDict):
    """LangGraph Agentì˜ ìƒíƒœ"""
    messages: Annotated[Sequence[BaseMessage], add_messages]
    user_query: str
    query_analysis: Optional[dict]
    retrieved_docs: Optional[List[Document]]
    generated_answer: Optional[str]
    next_action: Optional[str]
    evaluation_result: Optional[dict]
    retry_count: Optional[int]


# ============================================================
# [SECTION 4] Reranker - ì»¤ìŠ¤í…€ Jina Reranker Wrapper
# ============================================================
class JinaReranker(BaseDocumentCompressor):
    """Jina Reranker Wrapper for LangChain"""
    model_name: str = "jinaai/jina-reranker-v2-base-multilingual"
    top_n: int = 7
    model: Any = None
    tokenizer: Any = None

    class Config:
        arbitrary_types_allowed = True
        extra = "allow"

    def __init__(self, model_name: Optional[str] = None, top_n: Optional[int] = None, **kwargs):
        super().__init__(**kwargs)
        if model_name:
            self.model_name = model_name
        if top_n:
            self.top_n = top_n

        # Device selection: CUDA > MPS > CPU
        if torch.cuda.is_available():
            self.device = "cuda"
        elif torch.backends.mps.is_available():
            self.device = "mps"
        else:
            self.device = "cpu"

        logger.info(f"Loading Reranker: {self.model_name} on {self.device}")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name, trust_remote_code=True)
        # ëª¨ë¸ ì–‘ìí™” (FP16) - optimization #8
        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name, 
            trust_remote_code=True, 
            torch_dtype=torch.float16 if self.device != "cpu" else torch.float32
        )
        self.model.to(self.device)
        self.model.eval()
        logger.info("Reranker loaded successfully")

    def compress_documents(
        self, documents: Sequence[Document], query: str, callbacks: Optional[Any] = None
    ) -> Sequence[Document]:
        if not documents:
            return []

        pairs = [[query, doc.page_content] for doc in documents]

        with torch.no_grad():
            inputs = self.tokenizer(
                pairs, padding=True, truncation=True,
                return_tensors="pt", max_length=512
            )
            # Move inputs to device
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            scores = self.model(**inputs).logits.squeeze(-1).float().cpu()
            scores = torch.sigmoid(scores).tolist()
            if not isinstance(scores, list):
                scores = [scores]

        # Sort and select top_n
        top_indices = sorted(
            range(len(scores)), key=lambda i: scores[i], reverse=True
        )[:self.top_n]

        final_docs = []
        for i in top_indices:
            doc = documents[i]
            doc.metadata["relevance_score"] = scores[i]
            final_docs.append(doc)

        return final_docs


# ============================================================
# [SECTION 5] Pydantic Schemas - LLM êµ¬ì¡°í™”ëœ ì¶œë ¥ìš©
# ============================================================
class HybridQuery(BaseModel):
    """HyDE + Hybrid Searchë¥¼ ìœ„í•œ ì¿¼ë¦¬ í™•ì¥ ê²°ê³¼"""
    keyword_query: str = Field(
        description="BM25 ê²€ìƒ‰ìš©: ì¡°ì‚¬ ì œê±°ëœ í•µì‹¬ ë²•ë¥  í‚¤ì›Œë“œ (ì˜ˆ: 'ê·¼ë¡œê¸°ì¤€ë²• í•´ê³ ì˜ˆê³ ìˆ˜ë‹¹ ë¶€ë‹¹í•´ê³ ')")
    semantic_query: str = Field(
        description="Vector ê²€ìƒ‰ìš©: ì§ˆë¬¸ ì˜ë„ì™€ ë¬¸ë§¥ì„ í¬í•¨í•œ ìì—°ì–´ ë¬¸ì¥")
    hyde_passage: str = Field(
        description="Vector ê²€ìƒ‰ìš© ê°€ìƒ ë¬¸ì„œ: ì˜ˆìƒë˜ëŠ” ë²•ì¡°ë¬¸ ë‚´ìš© (2-3ë¬¸ì¥)")


class QueryAnalysis(BaseModel):
    """ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼"""
    category: str = Field(description="ë²•ë¥  ë¶„ì•¼: ë…¸ë™ë²•, í˜•ì‚¬ë²•, ë¯¼ì‚¬ë²•, ê¸°íƒ€")
    intent_type: str = Field(description="ì§ˆë¬¸ ì˜ë„: ë²•ë ¹ì¡°íšŒ, ì ˆì°¨ë¬¸ì˜, ìƒí™©íŒë‹¨, ê¶Œë¦¬í™•ì¸, ë¶„ìŸí•´ê²°, ì¼ë°˜ìƒë‹´")
    needs_clarification: bool = Field(default=False, description="ì§ˆë¬¸ ëª¨í˜¸ ì—¬ë¶€")
    needs_case_law: bool = Field(default=False, description="íŒë¡€ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€")
    query_complexity: str = Field(default="medium", description="ì§ˆë¬¸ ë‚œì´ë„: simple, medium, complex")
    clarification_question: str = Field(default="", description="ëª…í™•í™” ì§ˆë¬¸")
    user_situation: str = Field(default="", description="ì‚¬ìš©ì ìƒí™© ìš”ì•½")
    core_question: str = Field(default="", description="í•µì‹¬ ì§ˆë¬¸")
    related_laws: List[str] = Field(default_factory=list, description="ê´€ë ¨ ë²•ë¥ ëª…")


class AnswerEvaluation(BaseModel):
    """ë‹µë³€ í‰ê°€ ê²°ê³¼"""
    has_legal_basis: bool = Field(description="ë²•ì  ê·¼ê±° ëª…ì‹œ ì—¬ë¶€")
    cites_retrieved_docs: bool = Field(description="ê²€ìƒ‰ ë¬¸ì„œ ì¸ìš© ì—¬ë¶€")
    is_relevant: bool = Field(description="ë‹µë³€ ì í•©ì„±")
    needs_more_search: bool = Field(description="ì¶”ê°€ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€")
    quality_score: int = Field(description="í’ˆì§ˆ ì ìˆ˜ (1-5)")
    improvement_suggestion: str = Field(default="", description="ê°œì„  ì œì•ˆ")


# ============================================================
# [SECTION 6] Infrastructure Layer - ì™¸ë¶€ ë¦¬ì†ŒìŠ¤ ì—°ê²°
# ============================================================
class VectorStoreManager:
    """Qdrant ë²¡í„°ìŠ¤í† ì–´ ê´€ë¦¬ (Async Support)"""

    def __init__(self, config: Config):
        self.config = config
        self._load_env()
        self.embeddings = None
        self.client = None

    def _load_env(self):
        """í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ"""
        self.collection_name = os.getenv("QDRANT_COLLECTION_NAME")
        self.qdrant_url = os.getenv("QDRANT_URL")
        self.qdrant_api_key = os.getenv("QDRANT_API_KEY")

        if not self.qdrant_api_key:
            raise ValueError("QDRANT_API_KEYê°€ .envì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")

    def initialize(self):
        """ì„ë² ë”© ëª¨ë¸ë§Œ ì´ˆê¸°í™” (Qdrant ì—°ê²°ì€ Lazy Loading)"""
        logger.info(f"Loading embedding model: {self.config.EMBEDDING_MODEL}")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=self.config.EMBEDDING_MODEL,
            model_kwargs={'trust_remote_code': True},
            encode_kwargs={'normalize_embeddings': True}
        )
        logger.info("Embedding model loaded")
        
        # Qdrant ì—°ê²°ì€ ì‹¤ì œ ìš”ì²­ ì‹œ ìˆ˜í–‰ (ì´ë²¤íŠ¸ ë£¨í”„ ì¶©ëŒ ë°©ì§€)
        logger.info("Qdrant client will be initialized lazily on first request.")

    async def get_client(self) -> AsyncQdrantClient:
        """Qdrant Client Lazy Loading"""
        if self.client is None:
            logger.info("Connecting to Qdrant (Async) - Lazy Loading...")
            warnings.filterwarnings(
                'ignore', message='Api key is used with an insecure connection')
            
            self.client = AsyncQdrantClient(
                url=self.qdrant_url,
                api_key=self.qdrant_api_key,
                timeout=self.config.QDRANT_TIMEOUT,
                prefer_grpc=self.config.QDRANT_PREFER_GRPC
            )
            logger.info("Qdrant (Async) connected")
            
        return self.client

    def get_embeddings(self) -> HuggingFaceEmbeddings:
        if self.embeddings is None:
            raise ValueError("Embeddings model is not initialized. Call initialize() first.")
        return self.embeddings

    def get_collection_name(self) -> str:
        if self.collection_name is None:
            raise ValueError("Collection name is not set in environment variables.")
        return self.collection_name


class SparseEmbeddingManager:
    """Sparse Embedding (BGE-M3) ê´€ë¦¬"""

    def __init__(self, config: Config):
        self.config = config
        self.model = None

    def initialize(self):
        """BGE-M3 ëª¨ë¸ ë¡œë”© (Sparse)"""
        try:
            logger.info(
                f"Loading Sparse Model: {self.config.SPARSE_EMBEDDING_MODEL}")

            # Device check (Auto)
            use_fp16 = torch.cuda.is_available()
            self.model = BGEM3FlagModel(
                self.config.SPARSE_EMBEDDING_MODEL,
                use_fp16=use_fp16
            )
            logger.info("Sparse Model loaded (BGE-M3)")
        except Exception as e:
            logger.error(f"Failed to load Sparse Model: {e}")
            self.model = None

    def encode_query(self, query: str) -> Optional[models.SparseVector]:
        """ì¿¼ë¦¬ë¥¼ Sparse Vectorë¡œ ë³€í™˜"""
        if not self.model:
            return None

        try:
            # BGE-M3 encode returns dict with 'lexical_weights'
            output = self.model.encode(
                query,
                return_dense=False,
                return_sparse=True,
                return_colbert_vecs=False
            )
            # Dict[str, float] where str is token_id
            weights = output['lexical_weights']
            if not isinstance(weights, dict):
                 weights = {}

            return models.SparseVector(
                indices=list(map(int, weights.keys())),
                values=list(map(float, weights.values()))
            )
        except Exception as e:
            logger.error(f"Sparse encoding failed: {e}")
            return None


# ============================================================
# [SECTION 7] Logic Layer - LangGraph ë…¸ë“œ ë° ì›Œí¬í”Œë¡œìš° êµ¬ì„±
# ============================================================
class LegalRAGBuilder:
    """ë²•ë¥  RAG ê·¸ë˜í”„ ë¹Œë” (Async)"""

    def __init__(self, config: Config):
        self.config = config
        self.llm = None
        self.embeddings = None
        self.client = None  # AsyncQdrantClient
        self.sparse_manager = None
        self.query_expander = None
        self.reranker = None
        self.vs_manager = None

    def set_components(self, vs_manager: 'VectorStoreManager', reranker: 'JinaReranker'):
        """ë¯¸ë¦¬ ë¡œë”©ëœ ì»´í¬ë„ŒíŠ¸ ì£¼ì…"""
        self.vs_manager = vs_manager
        self.reranker = reranker

    def _init_infrastructure(self):
        """ì¸í”„ë¼ ì´ˆê¸°í™”"""
        # Vector Store Manager (Async)
        if not self.vs_manager:
            self.vs_manager = VectorStoreManager(self.config)
            self.vs_manager.initialize()
        
        self.embeddings = self.vs_manager.get_embeddings()

        # Sparse Embedding Manager
        if not self.sparse_manager:
            self.sparse_manager = SparseEmbeddingManager(self.config)
            self.sparse_manager.initialize()

        # LLM
        logger.info(f"Initializing LLM: {self.config.LLM_MODEL}")
        self.llm = ChatOpenAI(
            model=self.config.LLM_MODEL,
            temperature=self.config.LLM_TEMPERATURE,
            streaming=True
        )

        # Query Expander
        self.query_expander = self._create_query_expander()

        # Reranker
        if not self.reranker:
            self.reranker = JinaReranker(
                model_name=self.config.RERANKER_MODEL,
                top_n=self.config.TOP_K_RERANK
            )

    @traceable(run_type="retriever", name="Qdrant Hybrid Search")
    async def _execute_search(self, client: AsyncQdrantClient, dense_vec: List[float], sparse_vec: Optional[models.SparseVector], collection_name: str, limit: int) -> List[Document]:
        """Qdrant ê²€ìƒ‰ ìˆ˜í–‰ (LangSmith ì¶”ì ìš©)"""
        prefetch = [
            models.Prefetch(
                query=dense_vec,
                using="dense",
                limit=limit,
            )
        ]

        if sparse_vec:
            prefetch.append(
                models.Prefetch(
                    query=sparse_vec,
                    using="sparse",
                    limit=limit,
                )
            )

        # Execute Search
        results = await client.query_points(
            collection_name=collection_name,
            prefetch=prefetch,
            query=models.FusionQuery(fusion=models.Fusion.RRF),
            limit=limit
        )

        # Convert to Documents
        vector_docs = []
        for point in results.points:
            payload = point.payload
            text = payload.get("text", "")
            if text:
                doc = Document(
                    page_content=text,
                    metadata={k: v for k, v in payload.items()
                                if k != "text"}
                )
                doc.metadata["relevance_score"] = point.score
                vector_docs.append(doc)
        
        return vector_docs


    def _create_query_expander(self):
        """Query Expander ìƒì„± [ì‚¬ìš© í”„ë¡¬í”„íŠ¸: PROMPT_QUERY_EXPANSION] - HyDE + Hybrid"""
        structured_llm = self.llm.with_structured_output(HybridQuery)

        expansion_prompt = ChatPromptTemplate.from_messages([
            ("system", self.config.PROMPT_QUERY_EXPANSION),
            ("human", "{query}")
        ])

        async def expand_query(query: str) -> HybridQuery:
            try:
                # Async invoke
                chain = expansion_prompt | structured_llm
                # Type hint for IDE
                result: HybridQuery = await chain.ainvoke({"query": query})  # type: ignore
                logger.info(
                    f"HyDE Query Generated - Keyword: {result.keyword_query[:40]}...")
                return result
            except Exception as e:
                logger.warning(f"Query expansion failed: {e}")
                # Fallback
                return HybridQuery(
                    keyword_query=query,
                    semantic_query=query,
                    hyde_passage=query
                )

        return expand_query

    # --- Nodes (Async) ---

    def _create_analyze_node(self):
        """[ë…¸ë“œ: Analyze] ì§ˆë¬¸ ë¶„ì„ ë…¸ë“œ (Async)"""
        structured_llm = self.llm.with_structured_output(QueryAnalysis)

        analyze_prompt = ChatPromptTemplate.from_messages([
            ("system", self.config.PROMPT_ANALYZE),
            ("human", "{query}")
        ])

        async def analyze_query(state: AgentState) -> dict:
            query = state["user_query"]
            logger.info(f"Analyzing query: {query[:50]}...")

            chain = analyze_prompt | structured_llm
            analysis: QueryAnalysis = await chain.ainvoke({"query": query})  # type: ignore

            logger.info(
                f"Analysis: category={analysis.category}, intent={analysis.intent_type}")

            return {"query_analysis": analysis.model_dump()}

        return analyze_query

    def _create_clarify_node(self):
        """[ë…¸ë“œ: Clarify] ëª…í™•í™” ìš”ì²­ ë…¸ë“œ"""
        template = self.config.TEMPLATE_CLARIFY

        async def request_clarification(state: AgentState) -> dict:
            analysis = state.get("query_analysis", {})
            clarification_q = analysis.get(
                "clarification_question", "ì§ˆë¬¸ì„ ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ì£¼ì‹œê² ì–´ìš”?")

            answer = template.format(clarification_question=clarification_q)
            return {"generated_answer": answer, "next_action": "end"}

        return request_clarification

    def _create_search_node(self):
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë…¸ë“œ (Async + Qdrant Native Hybrid)"""
        # client = self.client  # ì—¬ê¸°ì„œëŠ” clientë¥¼ ë¯¸ë¦¬ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ (Lazy)
        embeddings = self.embeddings
        sparse_manager = self.sparse_manager
        query_expander = self.query_expander
        reranker = self.reranker
        config = self.config
        collection_name = self.vs_manager.get_collection_name()

        async def search_documents(state: AgentState) -> dict:
            original_query = state["user_query"]
            analysis = state.get("query_analysis", {})
            related_laws = analysis.get("related_laws", [])

            # 0. Get Client (Lazy Loading)
            client = await self.vs_manager.get_client()

            # 1. Query Expansion (Async)
            keyword_query = original_query
            vector_query = original_query

            if query_expander:
                hybrid = await query_expander(original_query)
                keyword_query = hybrid.keyword_query
                # Dense: HyDE ìš°ì„ , ì—†ìœ¼ë©´ semantic_query
                vector_query = hybrid.hyde_passage if hybrid.hyde_passage else hybrid.semantic_query

                logger.info(f"[Query] Keyword(Sparse): {keyword_query}")
                logger.info(f"[Query] Vector(Dense): {vector_query[:50]}...")

            # 2. Embedding Generation (Parallel: Dense + Sparse)
            # Embedding computation is CPU bound, run in thread if needed,
            # but usually fast enough or we can use asyncio.to_thread

            async def get_dense_vec():
                return await asyncio.to_thread(embeddings.embed_query, vector_query)

            async def get_sparse_vec():
                if sparse_manager:
                    return await asyncio.to_thread(sparse_manager.encode_query, keyword_query)
                return None

            dense_vec, sparse_vec = await asyncio.gather(get_dense_vec(), get_sparse_vec())

            # 3. Qdrant Native Hybrid Search (Traced)
            try:
                vector_docs = await self._execute_search(
                    client=client,
                    dense_vec=dense_vec,
                    sparse_vec=sparse_vec,
                    collection_name=collection_name,
                    limit=config.TOP_K_VECTOR
                )
                
                logger.info(f"Hybrid Search Results: {len(vector_docs)} docs")

            except Exception as e:
                logger.error(f"Search failed: {e}")
                import traceback
                traceback.print_exc()
                return {"retrieved_docs": []}

            # 4. Reranking (Async wrap or sync)
            if not vector_docs:
                return {"retrieved_docs": []}

            # Reranker logic (Sync) inside Async
            def rerank_logic(docs, query):
                if not reranker:
                    return docs
                return reranker.compress_documents(docs, query)

            reranked_docs = await asyncio.to_thread(rerank_logic, vector_docs, original_query)

            # 5. Filtering & Boosting
            final_docs = []
            for doc in reranked_docs:
                score = doc.metadata.get('relevance_score', 0)

                # Boosting
                law_name = doc.metadata.get('law_name', '')
                for rel_law in related_laws:
                    if rel_law in law_name:
                        score += 0.1
                        doc.metadata['boosted'] = True
                        break

                if score >= config.RELEVANCE_THRESHOLD:
                    final_docs.append(doc)

            # Sort and Slice
            final_docs.sort(key=lambda x: x.metadata.get(
                'relevance_score', 0), reverse=True)
            final_docs = final_docs[:config.TOP_K_FINAL]

            logger.info(f"Final selected: {len(final_docs)} docs")
            for i, doc in enumerate(final_docs, 1):
                meta = doc.metadata
                law = meta.get('law_name', 'ë²•ë ¹ëª…')
                art = meta.get('article_no', '')
                title = meta.get('article_title', '') or meta.get('title', '')
                score = meta.get('relevance_score', 0)
                logger.info(f"   [{i}] {law} ì œ{art}ì¡° {title} (Score: {score:.4f})")

            return {"retrieved_docs": final_docs}

        return search_documents

    def _create_generate_node(self):
        """[ë…¸ë“œ: Generate] ë‹µë³€ ìƒì„± ë…¸ë“œ (Async)"""
        llm = self.llm
        system_prompt = self.config.PROMPT_GENERATE
        no_results_template = self.config.TEMPLATE_NO_RESULTS

        answer_prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", """ì‚¬ìš©ì ì§ˆë¬¸: {query}

ğŸ“š ê²€ìƒ‰ëœ ë²•ë ¹/ë¬¸ì„œ:
{context}

{case_law_notice}

ìœ„ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.""")
        ])

        async def generate_answer(state: AgentState) -> dict:
            query = state["user_query"]
            docs = state.get("retrieved_docs", [])
            analysis = state.get("query_analysis", {})
            needs_case_law = analysis.get("needs_case_law", False)

            logger.info("Generating answer...")

            # Format context
            if docs:
                context_parts = []
                for i, doc in enumerate(docs, 1):
                    meta = doc.metadata
                    law_name = meta.get("law_name", "")
                    article = meta.get("article_no", "")
                    title = meta.get(
                        "article_title", "") or meta.get("title", "")
                    content = doc.page_content[:800]

                    header = f"[ë¬¸ì„œ {i}]"
                    if law_name:
                        header += f" {law_name}"
                        if article:
                            header += f" ì œ{article}ì¡°"
                    if title:
                        header += f" - {title}"

                    context_parts.append(f"{header}\n{content}\n")

                context = "\n".join(context_parts)
            else:
                context = "(ê´€ë ¨ ë²•ë ¹ ë¬¸ì„œê°€ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤)"

            case_law_notice = ""
            if needs_case_law:
                case_law_notice = "âš ï¸ ì°¸ê³ : íŒë¡€ ê²€ìƒ‰ì´ í•„ìš”í•˜ë‚˜ í˜„ì¬ DBì— í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤."

            if not docs:
                answer = no_results_template
            else:
                chain = answer_prompt | llm
                response = await chain.ainvoke({
                    "query": query,
                    "context": context,
                    "case_law_notice": case_law_notice
                })
                answer = response.content

            logger.info("Answer generated")
            return {"generated_answer": answer}

        return generate_answer

    def _create_evaluate_node(self):
        """[ë…¸ë“œ: Evaluate] ë‹µë³€ í‰ê°€ ë…¸ë“œ (Async)"""
        structured_llm = self.llm.with_structured_output(AnswerEvaluation)

        evaluate_prompt = ChatPromptTemplate.from_messages([
            ("system", self.config.PROMPT_EVALUATE),
            ("human", """## ì§ˆë¬¸
{query}

## ê²€ìƒ‰ëœ ë¬¸ì„œ ìš”ì•½
{context_summary}

## ìƒì„±ëœ ë‹µë³€
{answer}

í‰ê°€í•´ì£¼ì„¸ìš”.""")
        ])

        async def evaluate_answer(state: AgentState) -> dict:
            query = state["user_query"]
            answer = state.get("generated_answer", "")
            docs = state.get("retrieved_docs", [])
            retry_count = state.get("retry_count", 0) or 0

            logger.info(f"Evaluating answer (attempt {retry_count + 1})")

            if docs:
                context_summary = "\n".join([
                    f"- {doc.metadata.get('law_name', 'ë¬¸ì„œ')}: {doc.page_content[:100]}..."
                    for doc in docs[:5]
                ])
            else:
                context_summary = "(ê²€ìƒ‰ëœ ë¬¸ì„œ ì—†ìŒ)"

            chain = evaluate_prompt | structured_llm
            evaluation: AnswerEvaluation = await chain.ainvoke({  # type: ignore
                "query": query,
                "context_summary": context_summary,
                "answer": answer
            })

            logger.info(
                f"Evaluation: score={evaluation.quality_score}, needs_more={evaluation.needs_more_search}")

            return {
                "evaluation_result": evaluation.model_dump(),
                "retry_count": retry_count + 1
            }

        return evaluate_answer

    # --- Routing ---

    def _route_after_analysis(self, state: AgentState) -> Literal["clarify", "search"]:
        analysis = state.get("query_analysis", {})
        if analysis.get("needs_clarification", False):
            return "clarify"
        return "search"

    def _route_after_evaluation(self, state: AgentState) -> Literal["search", "end"]:
        evaluation = state.get("evaluation_result", {})
        retry_count = state.get("retry_count", 0) or 0

        if retry_count >= self.config.MAX_RETRY:
            logger.warning("Max retry reached")
            return "end"

        if evaluation.get("needs_more_search", False) and evaluation.get("quality_score", 3) <= 2:
            logger.info("Retrying search...")
            return "search"

        return "end"
    
    def _route_after_generate(self, state: AgentState) -> Literal["evaluate", "end"]:
        """ë‹µë³€ ìƒì„± í›„ ë¼ìš°íŒ…: ë‚œì´ë„ì— ë”°ë¼ í‰ê°€ ë‹¨ê³„ ì¡°ê±´ë¶€ ì‹¤í–‰"""
        analysis = state.get("analysis", {})
        complexity = analysis.get("query_complexity", "medium")
        
       # simple ì§ˆë¬¸ì€ í‰ê°€ ê±´ë„ˆë›°ê³  ë°”ë¡œ ì¢…ë£Œ
        if complexity == "simple":
            logger.info("Simple query detected - skipping evaluation")
            return "end"
        
        # medium, complexëŠ” í‰ê°€ ì§„í–‰
        logger.info(f"Query complexity: {complexity} - proceeding to evaluation")
        return "evaluate"

    # --- Build Graph ---

    def build(self) -> CompiledStateGraph:
        """LangGraph ë¹Œë“œ"""
        self._init_infrastructure()

        builder = StateGraph(AgentState)

        # Nodes
        builder.add_node("analyze", self._create_analyze_node())
        builder.add_node("clarify", self._create_clarify_node())
        builder.add_node("search", self._create_search_node())
        builder.add_node("generate", self._create_generate_node())
        builder.add_node("evaluate", self._create_evaluate_node())

        # Edges
        builder.set_entry_point("analyze")

        builder.add_conditional_edges(
            "analyze",
            self._route_after_analysis,
            {
                "clarify": "clarify",
                "search": "search"
            }
        )

        builder.add_edge("clarify", END)
        builder.add_edge("search", "generate")
        
        # generate â†’ evaluate OR end (ë‚œì´ë„ì— ë”°ë¼ ì¡°ê±´ë¶€)
        builder.add_conditional_edges(
            "generate",
            self._route_after_generate,
            {"evaluate": "evaluate", "end": END}
        )

        builder.add_conditional_edges(
            "evaluate",
            self._route_after_evaluation,
            {"search": "search", "end": END}
        )

        return builder.compile()


# ============================================================
# [SECTION 8] Execution Layer - ì‹¤í–‰ ì§„ì…ì 
# ============================================================
async def main():
    print("ğŸš€ Legal RAG Chatbot V8 (Async/Hybrid) Starting...")

    config = Config()
    app = LegalRAGBuilder(config).build()

    # Test Query
    # initial_query = "í‡´ì§ê¸ˆ ì§€ê¸‰ ê¸°í•œê³¼ ì•ˆ ì¤¬ì„ ë•Œ ì‹ ê³  ë°©ë²• ì•Œë ¤ì¤˜"
    initial_query = "ê·¼ë¡œê³„ì•½ì„œ ë¯¸ì‘ì„±ì‹œ ë²Œê¸ˆì€ ì–¼ë§ˆì¸ê°€ìš”?"

    print(f"\nğŸ‘¤ ì§ˆë¬¸: {initial_query}\n")

    initial_state = {
        "messages": [HumanMessage(content=initial_query)],
        "user_query": initial_query,
        "retry_count": 0
    }

    try:
        result = await app.ainvoke(initial_state)

        print("\n" + "=" * 50)
        print("ğŸ¤– AI ë‹µë³€:")
        print("=" * 50)
        print(result.get("generated_answer", "ë‹µë³€ ìƒì„± ì‹¤íŒ¨"))
        print("=" * 50)

        evaluation = result.get("evaluation_result", {})
        print(f"ğŸ“Š í‰ê°€ ì ìˆ˜: {evaluation.get('quality_score')}ì ")

    except Exception as e:
        logger.error(f"Execution failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
