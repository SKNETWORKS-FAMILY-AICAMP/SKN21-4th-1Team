import asyncio
import logging
from typing import Literal, List, Optional
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document
from langgraph.graph import StateGraph, END
from langgraph.graph.state import CompiledStateGraph
from langsmith import traceable
from qdrant_client import AsyncQdrantClient, models

from .config import Config
from .schemas import AgentState, ExpandedQuery, QueryAnalysis, AnswerEvaluation
from .infrastructure import VectorStoreManager, SparseEmbeddingManager, JinaReranker
from . import prompts

# ë…¸ë™ë²• ëª©ë¡ imports for fuzzy matching
import difflib
import sys
import os

# ============================================================
# [SECTION 7] Logic Layer - LangGraph ë…¸ë“œ ë° ì›Œí¬í”Œë¡œìš° êµ¬ì„±
# ============================================================
logger = logging.getLogger("LegalRAG-V8")

# í˜„ì¬ íŒŒì¼ ìœ„ì¹˜: chat/ai_module/graph.py
# labor_laws_list.pyëŠ” í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ìˆìŒ
try:
    from chat.ai_module.labor_laws_list import LABOR_LAWS_UNIQUE
except ImportError:
    # ê²½ë¡œ ë¬¸ì œ ë°œìƒ ì‹œ ì²˜ë¦¬ (ë°±ì—…)
    sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
    from labor_laws_list import LABOR_LAWS_UNIQUE

# ============================================================
# [SECTION 7] Logic Layer - LangGraph ë…¸ë“œ ë° ì›Œí¬í”Œë¡œìš° êµ¬ì„±
# ============================================================
class LegalRAGBuilder:
    """ë²•ë¥  RAG ê·¸ë˜í”„ ë¹Œë” (Async)"""

    def __init__(self, config: Config):
        self.config = config
        self.llm = None
        self.embeddings = None
        self.client = None  # AsyncQdrantClient
        self.sparse_manager = None
        self.query_expander = None
        self.reranker = None
        self.vs_manager = None

    def set_components(self, vs_manager: 'VectorStoreManager', reranker: 'JinaReranker'):
        """ë¯¸ë¦¬ ë¡œë”©ëœ ì»´í¬ë„ŒíŠ¸ ì£¼ì…"""
        self.vs_manager = vs_manager
        self.reranker = reranker

    def _init_infrastructure(self):
        """ì¸í”„ë¼ ì´ˆê¸°í™”"""
        # Vector Store Manager (Async)
        if not self.vs_manager:
            self.vs_manager = VectorStoreManager(self.config)
            self.vs_manager.initialize()
        
        self.embeddings = self.vs_manager.get_embeddings()

        # Sparse Embedding Manager
        if not self.sparse_manager:
            self.sparse_manager = SparseEmbeddingManager(self.config)
            self.sparse_manager.initialize()

        # LLM
        logger.info(f"Initializing LLM: {self.config.LLM_MODEL}")
        self.llm = ChatOpenAI(
            model=self.config.LLM_MODEL,
            temperature=self.config.LLM_TEMPERATURE,
            streaming=True
        )

        # Query Expander
        self.query_expander = self._create_query_expander()

        # Reranker - if not injected
        if not self.reranker:
            self.reranker = JinaReranker(
                model_name=self.config.RERANKER_MODEL,
                top_n=self.config.TOP_K_RERANK
            )

    @traceable(run_type="retriever", name="Qdrant Hybrid Search")
    async def _execute_search(self, client: AsyncQdrantClient, dense_vec: List[float], sparse_vec: Optional[models.SparseVector], collection_name: str, limit: int) -> List[Document]:
        """Qdrant ê²€ìƒ‰ ìˆ˜í–‰ (LangSmith ì¶”ì ìš©)"""
        prefetch = [
            models.Prefetch(
                query=dense_vec,
                using="dense",
                limit=limit,
            )
        ]

        if sparse_vec:
            prefetch.append(
                models.Prefetch(
                    query=sparse_vec,
                    using="sparse",
                    limit=limit,
                )
            )

        # Execute Search
        results = await client.query_points(
            collection_name=collection_name,
            prefetch=prefetch,
            query=models.FusionQuery(fusion=models.Fusion.RRF),
            limit=limit
        )

        # Convert to Documents
        vector_docs = []
        for point in results.points:
            payload = point.payload
            text = payload.get("text", "")
            if text:
                doc = Document(
                    page_content=text,
                    metadata={k: v for k, v in payload.items()
                                if k != "text"}
                )
                doc.metadata["relevance_score"] = point.score
                vector_docs.append(doc)
        
        return vector_docs


    def _create_query_expander(self):
        """[Node A: Query Rewriting] Query Expander ìƒì„± (Multi-Query Expansion)"""
        structured_llm = self.llm.with_structured_output(ExpandedQuery)

        expansion_prompt = ChatPromptTemplate.from_messages([
            ("system", prompts.PROMPT_QUERY_EXPANSION),
            ("human", "{query}")
        ])

        async def expand_query(query: str) -> ExpandedQuery:
            try:
                # Async invoke
                chain = expansion_prompt | structured_llm
                # Type hint for IDE
                result: ExpandedQuery = await chain.ainvoke({"query": query})  # type: ignore
                logger.info(
                    f"[Node A] Expanded Queries: {result.expanded_queries}")
                return result
            except Exception as e:
                logger.warning(f"Query expansion failed: {e}")
                # Fallback
                return ExpandedQuery(
                    original_query=query,
                    keyword_query=query,
                    expanded_queries=[query]
                )

        return expand_query

    # --- Nodes (Async) ---

    def _create_analyze_node(self):
        """[Node B: Analyze] ì§ˆë¬¸ ë¶„ì„ ë° ëª¨í˜¸ì„± íŒë‹¨ (Ambiguity Router)"""
        structured_llm = self.llm.with_structured_output(QueryAnalysis)

        analyze_prompt = ChatPromptTemplate.from_messages([
            ("system", prompts.PROMPT_ANALYZE),
            MessagesPlaceholder(variable_name="messages"),
            ("human", "{query}")
        ])

        async def analyze_query(state: AgentState) -> dict:
            query = state["user_query"]
            logger.info(f"[Node B] Analyzing query: {query[:50]}...")

            chain = analyze_prompt | structured_llm
            analysis: QueryAnalysis = await chain.ainvoke({
                "query": query,
                "messages": state["messages"]
            })  # type: ignore

            logger.info(
                f"Analysis: category={analysis.category}, intent={analysis.intent_type}, Ambiguous={analysis.is_ambiguous}")

            return {"query_analysis": analysis.model_dump()}

        return analyze_query

    def _create_verify_law_node(self):
        """[ë…¸ë“œ: Verify Law] ë²•ë ¹ ì¡´ì¬ ì—¬ë¶€ ê²€ì¦ (Async)"""
        
        async def verify_law(state: AgentState) -> dict:
            analysis = state.get("query_analysis", {})
            related_laws = analysis.get("related_laws", [])
            query = state["user_query"]
            
            # 1. ë²•ë ¹ ì–¸ê¸‰ì´ ì—†ê±°ë‚˜ ë…¸ë™ë²•ì´ ì•„ë‹ˆë©´ íŒ¨ìŠ¤
            if not related_laws or analysis.get("category") != "ë…¸ë™ë²•":
                return {"next_action": "search"}

            target_law = related_laws[0]
            
            # --- [New] Clean Law Name (Suffix Removal) ---
            # "ê³ ìš©ë…¸ë™ë²• ì œ34ì¡°" -> "ê³ ìš©ë…¸ë™ë²•"
            original_target = target_law
            target_law = self._clean_law_name(target_law)
            
            logger.info(f"[Verify] Target: '{original_target}' -> Cleaned: '{target_law}'")
            
            # 2. ê²€ì¦ ë¡œì§ ê°œì„  (Fuzzy Match -> DB Check)
            verified_law = None
            
            # 2-1. Static List Check (Exact + Fuzzy with Threshold)
            if target_law in LABOR_LAWS_UNIQUE:
                verified_law = target_law
            else:
                # Fuzzy matching with ratio check
                matches = difflib.get_close_matches(target_law, LABOR_LAWS_UNIQUE, n=1, cutoff=0.4)
                if matches:
                    candidate = matches[0]
                    # Calculate exact ratio
                    ratio = difflib.SequenceMatcher(None, target_law, candidate).ratio()
                    logger.info(f"Fuzzy match candidate: {candidate}, Ratio: {ratio:.4f}")
                    
                    if ratio >= 0.8:
                        # High confidence: Auto-correct
                        verified_law = candidate
                        logger.info(f"Auto-correcting (High confidence): {target_law} -> {verified_law}")
                    else:
                        # Medium confidence (0.4 <= ratio < 0.8): Ambiguous -> Suggestion
                        # Will be handled in the 'else' block of 'if verified_law:'
                        logger.info(f"Ambiguous match (Medium confidence): {target_law} -> {candidate}")
                        pass
            
            # 2-2. DB Check (If not verified yet)
            if not verified_law:
                exists = await self.vs_manager.check_law_exists(target_law)
                if exists:
                    verified_law = target_law

            if verified_law:
                logger.info(f"Law verification passed: {verified_law}")
                
                # Update analysis with corrected law name if needed
                if verified_law != target_law:
                    logger.info(f"Correcting query: '{target_law}' -> '{verified_law}'")
                    new_query = query.replace(original_target, verified_law, 1)
                    analysis["related_laws"] = [verified_law]
                    
                    return {
                        "next_action": "search",
                        "user_query": new_query,
                        "query_analysis": analysis
                    }
                
                return {"next_action": "search"}
            else:
                logger.info(f"Law verification failed/ambiguous: {target_law}")
                
                # 3. ì œì•ˆ (ìœ ì‚¬ ë²•ë ¹ ì°¾ê¸°)
                # Matches are already calculated or can be re-fetched.
                # Use strict cutoff for suggestion to avoid noise, but here we want to explain ambiguity.
                suggestions = difflib.get_close_matches(target_law, LABOR_LAWS_UNIQUE, n=3, cutoff=0.4)
                
                if suggestions:
                    suggestion_str = ", ".join([f"**'{s}'**" for s in suggestions])
                    msg = f"ë§ì”€í•˜ì‹  **'{target_law}'**ì€(ëŠ”) ë°ì´í„°ë² ì´ìŠ¤ì— ì—†ì§€ë§Œ, ë¹„ìŠ·í•œ ë²•ë ¹ì´ ìˆìŠµë‹ˆë‹¤.\ní˜¹ì‹œ {suggestion_str}ì„(ë¥¼) ë§ì”€í•˜ì‹œëŠ” ê±´ê°€ìš”?"
                else:
                    msg = f"ì£„ì†¡í•©ë‹ˆë‹¤, ë§ì”€í•˜ì‹  **'{target_law}'**ì€(ëŠ”) í˜„ì¬ ë…¸ë™ë²• ë°ì´í„°ë² ì´ìŠ¤ì— ë“±ë¡ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì •í™•í•œ ë²•ë ¹ëª…ì„ í™•ì¸í•´ ì£¼ì‹œê² ì–´ìš”?"
                
                return {
                    "generated_answer": msg,
                    "next_action": "clarify_end" # End flow and show message
                }

        return verify_law

    def _create_clarify_node(self):
        """[Node C: Clarify] ì—­ì§ˆë¬¸ ìƒì„± ì—ì´ì „íŠ¸"""
        
        # 1. í…œí”Œë¦¿ ë°©ì‹ ëŒ€ì‹  LLM ìƒì„± ë°©ì‹ ì‚¬ìš© (TEMPLATE_CLARIFY_GENERATOR)
        clarify_prompt = ChatPromptTemplate.from_messages([
            ("system", prompts.TEMPLATE_CLARIFY_GENERATOR),
            MessagesPlaceholder(variable_name="messages"),
            ("human", "{query}")
        ])

        async def request_clarification(state: AgentState) -> dict:
            query = state["user_query"]
            analysis = state.get("query_analysis", {})
            missing_info_list = analysis.get("missing_info", [])
            missing_str = ", ".join(missing_info_list) if missing_info_list else "êµ¬ì²´ì ì¸ ì‚¬ì‹¤ê´€ê³„"
            
            logger.info(f"[Node C] Generating clarification for missing: {missing_str}")

            chain = clarify_prompt | self.llm
            response = await chain.ainvoke({
                "query": query,
                "messages": state["messages"],
                "missing_info": missing_str
            })
            
            return {"generated_answer": response.content, "next_action": "end"}

        return request_clarification

    def _create_search_node(self):
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë…¸ë“œ (Multi-Query Expansion + RRF)"""
        # client = self.client  # ì—¬ê¸°ì„œëŠ” clientë¥¼ ë¯¸ë¦¬ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ (Lazy)
        embeddings = self.embeddings
        sparse_manager = self.sparse_manager
        query_expander = self.query_expander
        reranker = self.reranker
        config = self.config
        
        async def search_documents(state: AgentState) -> dict:
            analysis = state.get("query_analysis", {})
            # [Fix] Use core_question (standalone) if available, otherwise user_query
            search_target = analysis.get("core_question")
            if not search_target:
                search_target = state["user_query"]
                
             # ì¹´í…Œê³ ë¦¬ê°€ ë…¸ë™ë²• ì™¸ì¸ ê²½ìš° ê²€ìƒ‰ ìµœì†Œí™” ë˜ëŠ” ê±´ë„ˆë›°ê¸°ì¸ë°,
            # Flowìƒ verify_law -> searchë¡œ ë„˜ì–´ì˜¨ ê²ƒì´ë¯€ë¡œ ê²€ìƒ‰ ì§„í–‰
                
            logger.info(f"Searching with query (Core): {search_target}")
            
            related_laws = analysis.get("related_laws", [])
            collection_name = self.vs_manager.get_collection_name()

            # 0. Create Client
            client = await self.vs_manager.create_client()

            all_results = []

            try:
                # 1. Query Expansion (Node A)
                expanded = await query_expander(search_target)
                
                # Multi-Query Search List
                search_queries = [expanded.keyword_query] # ê¸°ë³¸ í‚¤ì›Œë“œ ì¿¼ë¦¬
                search_queries.extend(expanded.expanded_queries) # í™•ì¥ëœ ì¿¼ë¦¬ë“¤
                
                # ì¤‘ë³µ ì œê±° ë° Top 3 ì œí•œ
                search_queries = list(dict.fromkeys(search_queries))[:4]
                logger.info(f"[Search] Executing multi-queries: {search_queries}")
                
                # 2. Parallel Search Execution
                async def perform_single_search(q_text):
                    # Embed
                    d_vec = await asyncio.to_thread(embeddings.embed_query, q_text)
                    s_vec = None
                    if sparse_manager:
                        s_vec = await asyncio.to_thread(sparse_manager.encode_query, q_text) # í‚¤ì›Œë“œ ì¿¼ë¦¬ëŠ” q_text ê·¸ëŒ€ë¡œ ì‚¬ìš© (Note: q_text might contain naturally formulated query, but split by spacer is handled inside encode_query?? No, expecting tokens. But SparseBM25 usually takes raw text. Checking logic assumed safe.)
                    
                    return await self._execute_search(
                        client=client,
                        dense_vec=d_vec,
                        sparse_vec=s_vec,
                        collection_name=collection_name,
                        limit=5 # ê° ì¿¼ë¦¬ë‹¹ 5ê°œë§Œ ê°€ì ¸ì™€ì„œ í•©ì¹¨
                    )

                tasks = [perform_single_search(q) for q in search_queries]
                results_lists = await asyncio.gather(*tasks)
                
                # 3. Merge & Deduplicate (Simple RRF concept or Score Max)
                unique_docs = {}
                for res_list in results_lists:
                    for doc in res_list:
                        # RRF style simple scoring or just Max score override
                        # ID ëŒ€ì‹  content hashë‚˜ metadataë¡œ ì¤‘ë³µ ì²´í¬
                        doc_id = doc.metadata.get('id') or doc.page_content[:20]
                        if doc_id not in unique_docs:
                            unique_docs[doc_id] = doc
                        else:
                            # ì´ë¯¸ ìˆìœ¼ë©´ ì ìˆ˜ ë” ë†’ì€ê±¸ë¡œ êµì²´ (Soft selection)
                            if doc.metadata.get('relevance_score', 0) > unique_docs[doc_id].metadata.get('relevance_score', 0):
                                unique_docs[doc_id] = doc
                
                vector_docs = list(unique_docs.values())
                logger.info(f"Merged Multi-Query Results: {len(vector_docs)} docs")

            except Exception as e:
                logger.error(f"Search failed: {e}")
                import traceback
                traceback.print_exc()
                return {"retrieved_docs": []}
            finally:
                if client:
                    await client.close()

            # 4. Reranking
            if not vector_docs:
                return {"retrieved_docs": []}

            # Reranker logic
            def rerank_logic(docs, query):
                if not reranker:
                    return docs
                return reranker.compress_documents(docs, query)

            reranked_docs = await asyncio.to_thread(rerank_logic, vector_docs, search_target) # Rerankingì€ ì›ë³¸ ì¿¼ë¦¬(Core) ê¸°ì¤€

            # 5. Filtering & Boosting
            final_docs = []
            for doc in reranked_docs:
                score = doc.metadata.get('relevance_score', 0)

                # Boosting logic (Same as before)
                law_name = doc.metadata.get('law_name', '')
                for rel_law in related_laws:
                    if rel_law in law_name:
                        score += 0.1
                        doc.metadata['boosted'] = True
                        break

                if score >= config.RELEVANCE_THRESHOLD:
                    final_docs.append(doc)

            # Sort and Slice
            final_docs.sort(key=lambda x: x.metadata.get(
                'relevance_score', 0), reverse=True)
            final_docs = final_docs[:config.TOP_K_FINAL]

            logger.info(f"Final selected: {len(final_docs)} docs")
            for i, doc in enumerate(final_docs, 1):
                meta = doc.metadata
                law = meta.get('law_name', 'ë²•ë ¹ëª…')
                art = meta.get('article_no', '')
                title = meta.get('article_title', '') or meta.get('title', '')
                score = meta.get('relevance_score', 0)
                logger.info(f"   [{i}] {law} ì œ{art}ì¡° {title} (Score: {score:.4f})")

            return {"retrieved_docs": final_docs}

        return search_documents

    def _create_generate_node(self):
        """[ë…¸ë“œ: Generate] ë‹µë³€ ìƒì„± ë…¸ë“œ (Async) - intentë³„ í”„ë¡¬í”„íŠ¸ ì ìš©"""
        llm = self.llm

        async def generate_answer(state: AgentState) -> dict:
            query = state["user_query"]
            docs = state.get("retrieved_docs", [])
            analysis = state.get("query_analysis", {})
            
            # ë¶„ì„ ê²°ê³¼ì—ì„œ ì •ë³´ ì¶”ì¶œ
            intent_type = analysis.get("intent_type", "ì¼ë°˜ìƒë‹´")
            category = analysis.get("category", "ë…¸ë™ë²•")
            needs_case_law = analysis.get("needs_case_law", False)

            logger.info(f"Generating answer for intent: {intent_type}, category: {category}")

            # Format context
            if docs:
                context_parts = []
                for i, doc in enumerate(docs, 1):
                    meta = doc.metadata
                    law_name = meta.get("law_name", "")
                    article = meta.get("article_no", "")
                    title = meta.get(
                        "article_title", "") or meta.get("title", "")
                    content = doc.page_content[:800]

                    header = f"[ë¬¸ì„œ {i}]"
                    if law_name:
                        header += f" {law_name}"
                        if article:
                            header += f" ì œ{article}ì¡°"
                    if title:
                        header += f" - {title}"

                    context_parts.append(f"{header}\n{content}\n")

                context = "\n".join(context_parts)
            else:
                context = "(ê´€ë ¨ ë²•ë ¹ ë¬¸ì„œê°€ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤)"

            case_law_notice = ""
            if needs_case_law:
                case_law_notice = "âš ï¸ ì°¸ê³ : íŒë¡€ ê²€ìƒ‰ì´ í•„ìš”í•˜ë‚˜ í˜„ì¬ DBì— í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤."

            # ========== í•µì‹¬ ë³€ê²½: intentë³„ í”„ë¡¬í”„íŠ¸ ì„ íƒ ==========
            if category == "ê¸°íƒ€(ì¼ìƒ)":
                # ì¼ìƒ ëŒ€í™”: PROMPT_DAILY_LIFE ì‚¬ìš© (Context ë¶ˆí•„ìš”)
                daily_prompt = ChatPromptTemplate.from_messages([
                    ("system", prompts.PROMPT_DAILY_LIFE),
                    MessagesPlaceholder(variable_name="messages"),
                    ("human", "{query}")
                ])
                chain = daily_prompt | llm
                response = await chain.ainvoke({
                    "messages": state["messages"],
                    "query": query
                })
                answer = response.content
                logger.info("Daily life conversation generated")

            elif category == "ë…¸ë™ë²• ì™¸":
                # ë…¸ë™ë²• ì™¸: ê²€ìƒ‰ ìƒëµí•˜ê³  ì•ˆë‚´ ë©”ì‹œì§€ ë°˜í™˜
                answer = self._generate_no_results_message(category, query, analysis)
                logger.info("Out of scope message generated")

            elif not docs:
                # ë…¸ë™ë²•ì´ì§€ë§Œ ë¬¸ì„œ ì—†ìŒ â†’ ì¼ë°˜ì ì¸ ê²€ìƒ‰ ì‹¤íŒ¨ ë©”ì‹œì§€
                answer = self._generate_no_results_message(category, query, analysis)
            
            else:
                # ë…¸ë™ë²• + ë¬¸ì„œ ìˆìŒ â†’ RAG ë‹µë³€ ìƒì„±
                system_prompt = self._select_prompt_by_intent(intent_type)
                
                answer_prompt = ChatPromptTemplate.from_messages([
                    ("system", system_prompt),
                    MessagesPlaceholder(variable_name="messages"),
                    ("human", """ì‚¬ìš©ì ì§ˆë¬¸: {query}

ğŸ“š ê²€ìƒ‰ëœ ë²•ë ¹/ë¬¸ì„œ:
{context}

{case_law_notice}

ìœ„ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.""")
                ])

                chain = answer_prompt | llm
                response = await chain.ainvoke({
                    "messages": state["messages"],
                    "query": query,
                    "context": context,
                    "case_law_notice": case_law_notice
                })
                answer = response.content
                logger.info("Legal RAG Answer generated")

            return {"generated_answer": answer}

        return generate_answer

    def _create_evaluate_node(self):
        """[ë…¸ë“œ: Evaluate] ë‹µë³€ í‰ê°€ ë…¸ë“œ (Async)"""
        structured_llm = self.llm.with_structured_output(AnswerEvaluation)

        evaluate_prompt = ChatPromptTemplate.from_messages([
            ("system", prompts.PROMPT_EVALUATE),
            ("human", """## ì§ˆë¬¸
{query}

## ê²€ìƒ‰ëœ ë¬¸ì„œ ìš”ì•½
{context_summary}

## ìƒì„±ëœ ë‹µë³€
{answer}

í‰ê°€í•´ì£¼ì„¸ìš”.""")
        ])

        async def evaluate_answer(state: AgentState) -> dict:
            query = state["user_query"]
            answer = state.get("generated_answer", "")
            docs = state.get("retrieved_docs", [])
            retry_count = state.get("retry_count", 0) or 0

            logger.info(f"Evaluating answer (attempt {retry_count + 1})")

            if docs:
                context_summary = "\n".join([
                    f"- {doc.metadata.get('law_name', 'ë¬¸ì„œ')}: {doc.page_content[:100]}..."
                    for doc in docs[:5]
                ])
            else:
                context_summary = "(ê²€ìƒ‰ëœ ë¬¸ì„œ ì—†ìŒ)"

            chain = evaluate_prompt | structured_llm
            evaluation: AnswerEvaluation = await chain.ainvoke({  # type: ignore
                "query": query,
                "context_summary": context_summary,
                "answer": answer
            })

            logger.info(
                f"Evaluation: score={evaluation.quality_score}, needs_more={evaluation.needs_more_search}")

            return {
                "evaluation_result": evaluation.model_dump(),
                "retry_count": retry_count + 1
            }

        return evaluate_answer

    # --- Helper Methods for Prompt Selection ---

    def _select_prompt_by_intent(self, intent_type: str) -> str:
        """intent_typeì— ë”°ë¼ ì ì ˆí•œ í”„ë¡¬í”„íŠ¸ ì„ íƒ"""
        prompt_map = {
            "ë²•ë ¹ì¡°íšŒ": prompts.PROMPT_GENERATE_LAW_LOOKUP,
            "ì ˆì°¨ë¬¸ì˜": prompts.PROMPT_GENERATE_PROCEDURE,
            "ìƒí™©íŒë‹¨": prompts.PROMPT_GENERATE_SITUATION,
            "ê¶Œë¦¬í™•ì¸": prompts.PROMPT_GENERATE_RIGHTS,
            "ë¶„ìŸí•´ê²°": prompts.PROMPT_GENERATE_DISPUTE,
            "ì¼ë°˜ìƒë‹´": prompts.PROMPT_GENERATE
        }
        selected = prompt_map.get(intent_type, prompts.PROMPT_GENERATE)
        logger.info(f"Selected prompt for intent '{intent_type}'")
        return selected

    def _generate_no_results_message(self, category: str, query: str, analysis: dict) -> str:
        """ë‹µë³€ ë¶ˆê°€ ì‹œ ë§ì¶¤í˜• ë©”ì‹œì§€ ìƒì„±"""
        
        # ë…¸ë™ë²• ì™¸ ë¶„ì•¼ ì§ˆë¬¸ì¸ ê²½ìš°
        if category != "ë…¸ë™ë²•":
            # ì§ˆë¬¸ì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ ì‹œë„
            detected_law = self._extract_law_name(query, analysis)
            
            return prompts.TEMPLATE_NO_RESULTS_OUT_OF_SCOPE.format(
                detected_law=detected_law
            )
        
        # ë…¸ë™ë²•ì´ì§€ë§Œ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ
        return prompts.TEMPLATE_NO_RESULTS_NO_DOCS.format(query=query)

    def _extract_law_name(self, query: str, analysis: dict) -> str:
        """ì§ˆë¬¸ì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)"""
        import re
        
        # related_lawsì—ì„œ ì¶”ì¶œ
        related_laws = analysis.get("related_laws", [])
        if related_laws:
            return related_laws[0]
        
        # ì§ˆë¬¸ í…ìŠ¤íŠ¸ì—ì„œ "~ë²•" íŒ¨í„´ ì°¾ê¸°
        law_pattern = r'([ê°€-í£]+ë²•)'
        matches = re.findall(law_pattern, query)
        if matches:
            return matches[0]
        
        # ê¸°ë³¸ê°’
        return "í•´ë‹¹ ë²•ë ¹"

    def _clean_law_name(self, text: str) -> str:
        """ë²•ë ¹ëª…ì—ì„œ 'ì œXXì¡°' ë“± ì¡°í•­ ë²ˆí˜¸ ì œê±°"""
        import re
        # Pattern: (ê³µë°±)ì œ(ê³µë°±)ìˆ«ì(ê³µë°±)(ì¡°|í•­|í˜¸|ëª©)... ëê¹Œì§€
        pattern = r'\s*ì œ\s*\d+\s*(ì¡°|í•­|í˜¸|ëª©).*$'
        cleaned = re.sub(pattern, '', text).strip()
        return cleaned

    # --- Routing ---

    def _route_after_analysis(self, state: AgentState) -> Literal["clarify", "search", "generate", "verify_law"]:
        analysis = state.get("query_analysis", {})
        category = analysis.get("category", "ë…¸ë™ë²•")
        
        # [Node B Logic] Ambiguous -> Clarify
        if analysis.get("is_ambiguous", False):
            return "clarify"
            
        # ê¸°ì¡´ ë¡œì§: needs_clarification (deprecated logic, but keeping for safely)
        if analysis.get("needs_clarification", False):
            return "clarify"
        
        # ë…¸ë™ë²• ì™¸, ê¸°íƒ€(ì¼ìƒ)ì€ ê²€ìƒ‰ ìƒëµí•˜ê³  ë°”ë¡œ Generateë¡œ ì´ë™
        if category in ["ë…¸ë™ë²• ì™¸", "ê¸°íƒ€(ì¼ìƒ)"]:
            logger.info(f"Skipping search for category: {category}")
            return "generate"

        # ë…¸ë™ë²•ì¸ ê²½ìš° Law Verification ë‹¨ê³„ ê±°ì¹¨
        return "verify_law"

    def _route_after_verify(self, state: AgentState) -> Literal["search", "end"]:
        """ê²€ì¦ í›„ ë¼ìš°íŒ…"""
        next_action = state.get("next_action", "search")
        if next_action == "clarify_end":
            return "end" # ì´ë¯¸ generated_answerì— ì•ˆë‚´ ë©”ì‹œì§€ê°€ ìˆìŒ
        return "search"

    def _route_after_evaluation(self, state: AgentState) -> Literal["search", "end"]:
        evaluation = state.get("evaluation_result", {})
        retry_count = state.get("retry_count", 0) or 0

        if retry_count >= self.config.MAX_RETRY:
            logger.warning("Max retry reached")
            return "end"

        # Strict Pass Criteria: 3ê°€ì§€ ê¸°ì¤€ ì¤‘ í•˜ë‚˜ë¼ë„ Falseë©´ ì¬ê²€ìƒ‰
        # ë‹¨, needs_more_searchê°€ ëª…ì‹œì ìœ¼ë¡œ Trueì¸ ê²½ìš°ë„ í¬í•¨
        is_perfect = (
            evaluation.get("has_legal_basis", False) and
            evaluation.get("cites_retrieved_docs", False) and
            evaluation.get("is_relevant", False)
        )

        if not is_perfect or evaluation.get("needs_more_search", False):
            logger.info(f"Evaluation failed (Perfect={is_perfect}). Retrying search...")
            return "search"

        return "end"
    
    def _route_after_generate(self, state: AgentState) -> Literal["evaluate", "end"]:
        """ë‹µë³€ ìƒì„± í›„ ë¼ìš°íŒ…: ë‚œì´ë„ ë° ì¹´í…Œê³ ë¦¬ì— ë”°ë¼ í‰ê°€ ë‹¨ê³„ ì¡°ê±´ë¶€ ì‹¤í–‰"""
        analysis = state.get("query_analysis", {}) 
        complexity = analysis.get("query_complexity", "medium")
        category = analysis.get("category", "ë…¸ë™ë²•")
        
        # 1. ë…¸ë™ë²•ì´ ì•„ë‹Œ ê²½ìš° (ì¼ìƒ, ë…¸ë™ë²• ì™¸) -> í‰ê°€ ìƒëµ
        if category != "ë…¸ë™ë²•":
            logger.info(f"Category '{category}' - skipping evaluation")
            return "end"

        # 2. simple ì§ˆë¬¸ì€ í‰ê°€ ê±´ë„ˆë›°ê³  ë°”ë¡œ ì¢…ë£Œ
        if complexity == "simple":
            logger.info("Simple query detected - skipping evaluation")
            return "end"
        
        # medium, complexëŠ” í‰ê°€ ì§„í–‰
        logger.info(f"Query complexity: {complexity} - proceeding to evaluation")
        return "evaluate"

    # --- Build Graph ---

    def build(self) -> CompiledStateGraph:
        """LangGraph ë¹Œë“œ"""
        self._init_infrastructure()

        builder = StateGraph(AgentState)

        # Nodes
        builder.add_node("analyze", self._create_analyze_node())
        builder.add_node("verify_law", self._create_verify_law_node())
        builder.add_node("clarify", self._create_clarify_node())
        builder.add_node("search", self._create_search_node())
        builder.add_node("generate", self._create_generate_node())
        builder.add_node("evaluate", self._create_evaluate_node())

        # Edges
        builder.set_entry_point("analyze")

        builder.add_conditional_edges(
            "analyze",
            self._route_after_analysis,
            {
                "clarify": "clarify",
                "search": "search",     # Fallback (old)
                "generate": "generate",
                "verify_law": "verify_law"
            }
        )

        builder.add_edge("clarify", END)
        
        # Verify Law -> Search or End
        builder.add_conditional_edges(
            "verify_law",
            self._route_after_verify,
            {
                "search": "search",
                "end": END
            }
        )
        
        builder.add_edge("search", "generate")
        
        # generate â†’ evaluate OR end (ë‚œì´ë„ì— ë”°ë¼ ì¡°ê±´ë¶€)
        builder.add_conditional_edges(
            "generate",
            self._route_after_generate,
            {"evaluate": "evaluate", "end": END}
        )

        builder.add_conditional_edges(
            "evaluate",
            self._route_after_evaluation,
            {"search": "search", "end": END}
        )

        return builder.compile()
