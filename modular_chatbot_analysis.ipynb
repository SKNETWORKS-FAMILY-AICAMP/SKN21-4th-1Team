{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ëª¨ë“ˆí™”ëœ ë²•ë¥  RAG ì±—ë´‡ ì½”ë“œ ë¶„ì„\n",
                "\n",
                "**ì‘ì„±ì¼**: 2026-01-29  \n",
                "**ëŒ€ìƒ ë””ë ‰í† ë¦¬**: `chat/ai_module/`  \n",
                "**ëª©ì **: ë¦¬íŒ©í† ë§ëœ ëª¨ë“ˆ êµ¬ì¡° ë¶„ì„ ë° ì„±ëŠ¥ ìµœì í™” ë°©ì•ˆ ì œì‹œ\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ“‹ ëª©ì°¨\n",
                "\n",
                "1. [ëª¨ë“ˆ êµ¬ì¡° ê°œìš”](#1-ëª¨ë“ˆ-êµ¬ì¡°-ê°œìš”)\n",
                "2. [íŒŒì¼ë³„ ìƒì„¸ ë¶„ì„](#2-íŒŒì¼ë³„-ìƒì„¸-ë¶„ì„)\n",
                "   - 2.1 [`__init__.py`](#21-__init__py)\n",
                "   - 2.2 [`config.py`](#22-configpy)\n",
                "   - 2.3 [`prompts.py`](#23-promptspy)\n",
                "   - 2.4 [`schemas.py`](#24-schemaspy)\n",
                "   - 2.5 [`infrastructure.py`](#25-infrastructurepy)\n",
                "   - 2.6 [`graph.py`](#26-graphpy)\n",
                "3. [íŒŒì¼ ê°„ ê´€ê³„ë„](#3-íŒŒì¼-ê°„-ê´€ê³„ë„)\n",
                "4. [ì„±ëŠ¥ ìµœì í™” ë°©ì•ˆ](#4-ì„±ëŠ¥-ìµœì í™”-ë°©ì•ˆ)\n",
                "5. [ìµœì í™” ì½”ë“œ ìˆ˜ì • ê°€ì´ë“œ](#5-ìµœì í™”-ì½”ë“œ-ìˆ˜ì •-ê°€ì´ë“œ)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. ëª¨ë“ˆ êµ¬ì¡° ê°œìš”\n",
                "\n",
                "ê¸°ì¡´ì˜ ë‹¨ì¼ íŒŒì¼(`chatbot_graph_V8_FINAL.py`, 965ì¤„)ì„ **6ê°œì˜ ëª¨ë“ˆ**ë¡œ ë¶„ë¦¬í•˜ì—¬ ìœ ì§€ë³´ìˆ˜ì„±ê³¼ í™•ì¥ì„±ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.\n",
                "\n",
                "### íŒŒì¼ êµ¬ì¡°\n",
                "\n",
                "```\n",
                "chat/ai_module/\n",
                "â”œâ”€â”€ __init__.py          (6ì¤„)   - íŒ¨í‚¤ì§€ ì§„ì…ì \n",
                "â”œâ”€â”€ config.py            (38ì¤„)  - ì„¤ì • ê´€ë¦¬\n",
                "â”œâ”€â”€ prompts.py           (98ì¤„)  - í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
                "â”œâ”€â”€ schemas.py           (56ì¤„)  - ë°ì´í„° ìŠ¤í‚¤ë§ˆ\n",
                "â”œâ”€â”€ infrastructure.py    (207ì¤„) - ì¸í”„ë¼ ì»´í¬ë„ŒíŠ¸\n",
                "â””â”€â”€ graph.py             (500ì¤„) - LangGraph ë¡œì§\n",
                "```\n",
                "\n",
                "### ì„¤ê³„ ì›ì¹™\n",
                "\n",
                "| ì›ì¹™ | ì„¤ëª… |\n",
                "|------|------|\n",
                "| **ë‹¨ì¼ ì±…ì„** | ê° íŒŒì¼ì´ í•˜ë‚˜ì˜ ëª…í™•í•œ ì—­í• ë§Œ ìˆ˜í–‰ |\n",
                "| **ì˜ì¡´ì„± ë¶„ë¦¬** | ì„¤ì • â†’ ìŠ¤í‚¤ë§ˆ â†’ ì¸í”„ë¼ â†’ ë¡œì§ ìˆœìœ¼ë¡œ ì˜ì¡´ |\n",
                "| **ì¬ì‚¬ìš©ì„±** | í”„ë¡¬í”„íŠ¸, ì„¤ì • ë“±ì„ ë…ë¦½ì ìœ¼ë¡œ ìˆ˜ì • ê°€ëŠ¥ |\n",
                "| **í…ŒìŠ¤íŠ¸ ìš©ì´ì„±** | ê° ëª¨ë“ˆì„ ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥ |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. íŒŒì¼ë³„ ìƒì„¸ ë¶„ì„\n",
                "\n",
                "### 2.1 `__init__.py`\n",
                "\n",
                "**ì—­í• **: íŒ¨í‚¤ì§€ì˜ **ê³µê°œ API** ì •ì˜\n",
                "\n",
                "```python\n",
                "from .config import Config\n",
                "from .infrastructure import VectorStoreManager, JinaReranker\n",
                "from .graph import LegalRAGBuilder\n",
                "\n",
                "__all__ = [\"Config\", \"VectorStoreManager\", \"JinaReranker\", \"LegalRAGBuilder\"]\n",
                "```\n",
                "\n",
                "#### í•µì‹¬ ê¸°ëŠ¥\n",
                "\n",
                "- **ì™¸ë¶€ ì¸í„°í˜ì´ìŠ¤**: ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œ `from chat.ai_module import Config` í˜•íƒœë¡œ ì„í¬íŠ¸ ê°€ëŠ¥\n",
                "- **ìº¡ìŠí™”**: ë‚´ë¶€ êµ¬í˜„(`prompts`, `schemas`)ì€ ìˆ¨ê¸°ê³  í•„ìš”í•œ ê²ƒë§Œ ë…¸ì¶œ\n",
                "\n",
                "#### ì‚¬ìš© ì˜ˆì‹œ\n",
                "\n",
                "```python\n",
                "# services.pyì—ì„œ ì‚¬ìš©\n",
                "from chat.ai_module import Config, LegalRAGBuilder, VectorStoreManager, JinaReranker\n",
                "\n",
                "config = Config()\n",
                "builder = LegalRAGBuilder(config)\n",
                "```\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 `config.py`\n",
                "\n",
                "**ì—­í• **: **ì¤‘ì•™ ì§‘ì¤‘ì‹ ì„¤ì • ê´€ë¦¬** (Django settings ì—°ë™)\n",
                "\n",
                "```python\n",
                "@dataclass\n",
                "class Config:\n",
                "    # Django settingsì—ì„œ ê°’ì„ ê°€ì ¸ì˜¤ë˜, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©\n",
                "    LLM_MODEL: str = getattr(settings, \"LLM_MODEL\", \"gpt-4o-mini\")\n",
                "    LLM_TEMPERATURE: float = getattr(settings, \"LLM_TEMPERATURE\", 0.0)\n",
                "    \n",
                "    TOP_K_VECTOR: int = getattr(settings, \"TOP_K_VECTOR\", 10)\n",
                "    TOP_K_RERANK: int = getattr(settings, \"TOP_K_RERANK\", 5)\n",
                "    TOP_K_FINAL: int = getattr(settings, \"TOP_K_FINAL\", 3)\n",
                "    \n",
                "    QDRANT_COLLECTION_NAME: str = getattr(settings, \"QDRANT_COLLECTION_NAME\", \n",
                "                                          os.getenv(\"QDRANT_COLLECTION_NAME\"))\n",
                "```\n",
                "\n",
                "#### í•µì‹¬ íŠ¹ì§•\n",
                "\n",
                "1. **Django í†µí•©**: `django.conf.settings`ì—ì„œ ê°’ì„ ìš°ì„  ê°€ì ¸ì˜´\n",
                "2. **í™˜ê²½ ë³€ìˆ˜ í´ë°±**: Django settingsì— ì—†ìœ¼ë©´ `.env`ì—ì„œ ê°€ì ¸ì˜´\n",
                "3. **íƒ€ì… ì•ˆì •ì„±**: `@dataclass`ë¡œ íƒ€ì… íŒíŠ¸ ì œê³µ\n",
                "\n",
                "#### ì„¤ì • í•­ëª© ë¶„ë¥˜\n",
                "\n",
                "| ì¹´í…Œê³ ë¦¬ | ì„¤ì • í•­ëª© | ê¸°ë³¸ê°’ |\n",
                "|----------|-----------|--------|\n",
                "| **ëª¨ë¸** | `LLM_MODEL` | `gpt-4o-mini` |\n",
                "| | `EMBEDDING_MODEL` | `Qwen/Qwen3-Embedding-0.6B` |\n",
                "| | `RERANKER_MODEL` | `jinaai/jina-reranker-v2-base-multilingual` |\n",
                "| **RAG** | `TOP_K_VECTOR` | 10 |\n",
                "| | `TOP_K_RERANK` | 5 |\n",
                "| | `TOP_K_FINAL` | 3 |\n",
                "| | `RELEVANCE_THRESHOLD` | 0.2 |\n",
                "| **Qdrant** | `QDRANT_TIMEOUT` | 10ì´ˆ |\n",
                "| | `QDRANT_PREFER_GRPC` | True |\n",
                "\n",
                "#### ì„±ëŠ¥ ê´€ë ¨ ì„¤ì •\n",
                "\n",
                "- **`TOP_K_VECTOR`**: ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (â†“ ê°’ â†’ â†‘ ì†ë„, â†“ ì¬í˜„ìœ¨)\n",
                "- **`QDRANT_TIMEOUT`**: Qdrant ì—°ê²° íƒ€ì„ì•„ì›ƒ (â†“ ê°’ â†’ â†‘ ì‘ë‹µì„±)\n",
                "- **`QDRANT_PREFER_GRPC`**: gRPC ì‚¬ìš© ì—¬ë¶€ (True â†’ ë” ë¹ ë¦„)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.3 `prompts.py`\n",
                "\n",
                "**ì—­í• **: **ëª¨ë“  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê´€ë¦¬**\n",
                "\n",
                "#### í”„ë¡¬í”„íŠ¸ ëª©ë¡\n",
                "\n",
                "| ë³€ìˆ˜ëª… | ì‚¬ìš© ë…¸ë“œ | ëª©ì  |\n",
                "|--------|-----------|------|\n",
                "| `PROMPT_QUERY_EXPANSION` | Query Expander | HyDE + Hybrid ì¿¼ë¦¬ ìƒì„± |\n",
                "| `PROMPT_ANALYZE` | Analyze | ì§ˆë¬¸ ë¶„ì„ (ì¹´í…Œê³ ë¦¬, ë‚œì´ë„ ë“±) |\n",
                "| `PROMPT_GENERATE` | Generate | ë‹µë³€ ìƒì„± (ì¸ìš© ê°•ì œ) |\n",
                "| `PROMPT_EVALUATE` | Evaluate | ë‹µë³€ í’ˆì§ˆ í‰ê°€ |\n",
                "| `TEMPLATE_CLARIFY` | Clarify | ëª…í™•í™” ìš”ì²­ ë©”ì‹œì§€ |\n",
                "| `TEMPLATE_NO_RESULTS` | Generate | ê²€ìƒ‰ ê²°ê³¼ ì—†ì„ ë•Œ |\n",
                "\n",
                "#### ì˜ˆì‹œ: `PROMPT_GENERATE`\n",
                "\n",
                "```python\n",
                "PROMPT_GENERATE = \"\"\"ë‹¹ì‹ ì€ ì—„ê²©í•œ ê¸°ì¤€ì„ ê°€ì§„ ë²•ë¥  AI 'A-TEAM'ì…ë‹ˆë‹¤.\n",
                "\n",
                "## í•µì‹¬ ì›ì¹™\n",
                "1. **ì¦ê±° ê¸°ë°˜**: ë°˜ë“œì‹œ ì œê³µëœ [ê²€ìƒ‰ëœ ë¬¸ì„œ]ì— ìˆëŠ” ë‚´ìš©ë§Œ ì‚¬ìš©\n",
                "2. **Hallucination ê¸ˆì§€**: ë¬¸ì„œì— ì—†ëŠ” ë²•ì¡°ë¬¸ì„ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”\n",
                "3. **ì—„ê²©í•œ ì¸ìš©**: ëª¨ë“  ì§„ìˆ  ë’¤ì— [1], [2] ì¶œì²˜ í‘œê¸°\n",
                "\n",
                "## ë‹µë³€ í˜•ì‹\n",
                "**ğŸ¤” ë¶„ì„**\n",
                "...\n",
                "\"\"\"\n",
                "```\n",
                "\n",
                "#### ì¥ì \n",
                "\n",
                "- **ë²„ì „ ê´€ë¦¬**: í”„ë¡¬í”„íŠ¸ ë³€ê²½ ì´ë ¥ ì¶”ì  ìš©ì´\n",
                "- **A/B í…ŒìŠ¤íŠ¸**: ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ ë²„ì „ ë¹„êµ ê°€ëŠ¥\n",
                "- **ì¬ì‚¬ìš©**: ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì—ì„œë„ í™œìš© ê°€ëŠ¥\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.4 `schemas.py`\n",
                "\n",
                "**ì—­í• **: **ë°ì´í„° êµ¬ì¡° ì •ì˜** (Pydantic + TypedDict)\n",
                "\n",
                "#### 1) `AgentState` (TypedDict)\n",
                "\n",
                "```python\n",
                "class AgentState(TypedDict):\n",
                "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
                "    user_query: str\n",
                "    query_analysis: Optional[dict]\n",
                "    retrieved_docs: Optional[List[Any]]\n",
                "    generated_answer: Optional[str]\n",
                "    evaluation_result: Optional[dict]\n",
                "    retry_count: Optional[int]\n",
                "```\n",
                "\n",
                "**ìš©ë„**: LangGraph ë…¸ë“œ ê°„ ì „ë‹¬ë˜ëŠ” ìƒíƒœ ì •ì˜\n",
                "\n",
                "#### 2) `HybridQuery` (Pydantic)\n",
                "\n",
                "```python\n",
                "class HybridQuery(BaseModel):\n",
                "    keyword_query: str  # Sparse ê²€ìƒ‰ìš©\n",
                "    semantic_query: str  # Dense ê²€ìƒ‰ìš©\n",
                "    hyde_passage: str  # HyDE ê°€ìƒ ë¬¸ì„œ\n",
                "```\n",
                "\n",
                "**ìš©ë„**: Query Expansion ë…¸ë“œì˜ LLM ì¶œë ¥ í˜•ì‹ ê°•ì œ\n",
                "\n",
                "#### 3) `QueryAnalysis` (Pydantic)\n",
                "\n",
                "```python\n",
                "class QueryAnalysis(BaseModel):\n",
                "    category: str  # ë…¸ë™ë²•, í˜•ì‚¬ë²•, ë¯¼ì‚¬ë²•, ê¸°íƒ€\n",
                "    intent_type: str  # ë²•ë ¹ì¡°íšŒ, ì ˆì°¨ë¬¸ì˜ ë“±\n",
                "    query_complexity: str  # simple, medium, complex\n",
                "    needs_clarification: bool\n",
                "    related_laws: List[str]\n",
                "```\n",
                "\n",
                "**ìš©ë„**: Analyze ë…¸ë“œì˜ ì¶œë ¥ êµ¬ì¡°í™”\n",
                "\n",
                "#### 4) `AnswerEvaluation` (Pydantic)\n",
                "\n",
                "```python\n",
                "class AnswerEvaluation(BaseModel):\n",
                "    has_legal_basis: bool\n",
                "    cites_retrieved_docs: bool\n",
                "    is_relevant: bool\n",
                "    needs_more_search: bool\n",
                "    quality_score: int  # 1-5ì \n",
                "```\n",
                "\n",
                "**ìš©ë„**: Evaluate ë…¸ë“œì˜ í‰ê°€ ê²°ê³¼ êµ¬ì¡°í™”\n",
                "\n",
                "#### ì™œ Pydanticì„ ì‚¬ìš©í•˜ëŠ”ê°€?\n",
                "\n",
                "- **LLM ì¶œë ¥ ê°•ì œ**: JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ë„ë¡ ì œì•½\n",
                "- **íŒŒì‹± ë¶ˆí•„ìš”**: ë¬¸ìì—´ íŒŒì‹± ì—†ì´ ë°”ë¡œ ê°ì²´ë¡œ ì‚¬ìš©\n",
                "- **íƒ€ì… ê²€ì¦**: ëŸ°íƒ€ì„ì— íƒ€ì… ì˜¤ë¥˜ ìë™ ê°ì§€\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.5 `infrastructure.py`\n",
                "\n",
                "**ì—­í• **: **ì™¸ë¶€ ë¦¬ì†ŒìŠ¤ ì—°ê²°** (Qdrant, ì„ë² ë”©, Reranker)\n",
                "\n",
                "#### í´ë˜ìŠ¤ êµ¬ì¡°\n",
                "\n",
                "```\n",
                "infrastructure.py\n",
                "â”œâ”€â”€ JinaReranker          (L20-L91)\n",
                "â”œâ”€â”€ VectorStoreManager    (L97-L155)\n",
                "â””â”€â”€ SparseEmbeddingManager (L158-L207)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "#### 1) `JinaReranker`\n",
                "\n",
                "```python\n",
                "class JinaReranker(BaseDocumentCompressor):\n",
                "    def __init__(self, model_name, top_n):\n",
                "        # Device ìë™ ì„ íƒ: CUDA > MPS > CPU\n",
                "        self.device = \"cuda\" if torch.cuda.is_available() else \"mps\" if ...\n",
                "        \n",
                "        # FP16 ì–‘ìí™”\n",
                "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
                "            model_name,\n",
                "            torch_dtype=torch.float16 if self.device != \"cpu\" else torch.float32\n",
                "        )\n",
                "    \n",
                "    def compress_documents(self, documents, query):\n",
                "        # [ì§ˆë¬¸, ë¬¸ì„œ] ìŒìœ¼ë¡œ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°\n",
                "        pairs = [[query, doc.page_content] for doc in documents]\n",
                "        scores = self.model(**inputs).logits\n",
                "        # ìƒìœ„ top_nê°œ ì„ íƒ\n",
                "        return sorted_docs[:self.top_n]\n",
                "```\n",
                "\n",
                "**ìµœì í™” í¬ì¸íŠ¸**:\n",
                "- **FP16 ì–‘ìí™”** (L53): GPU ë©”ëª¨ë¦¬ 50% ì ˆì•½\n",
                "- **ë°°ì¹˜ ì²˜ë¦¬**: ëª¨ë“  ë¬¸ì„œë¥¼ í•œ ë²ˆì— ì²˜ë¦¬\n",
                "\n",
                "---\n",
                "\n",
                "#### 2) `VectorStoreManager`\n",
                "\n",
                "```python\n",
                "class VectorStoreManager:\n",
                "    def initialize(self):\n",
                "        # ì„ë² ë”© ëª¨ë¸ë§Œ ë¡œë”© (ë¬´ê±°ìš´ ì‘ì—…)\n",
                "        self.embeddings = HuggingFaceEmbeddings(\n",
                "            model_name=self.config.EMBEDDING_MODEL,\n",
                "            encode_kwargs={'normalize_embeddings': True}\n",
                "        )\n",
                "    \n",
                "    async def create_client(self) -> AsyncQdrantClient:\n",
                "        # ë§¤ ìš”ì²­ë§ˆë‹¤ ìƒˆ í´ë¼ì´ì–¸íŠ¸ ìƒì„± (Lazy Loading)\n",
                "        return AsyncQdrantClient(\n",
                "            url=self.qdrant_url,\n",
                "            timeout=self.config.QDRANT_TIMEOUT,\n",
                "            prefer_grpc=True  # gRPC ì‚¬ìš©\n",
                "        )\n",
                "```\n",
                "\n",
                "**ì„¤ê³„ í¬ì¸íŠ¸**:\n",
                "- **Lazy Loading**: Qdrant í´ë¼ì´ì–¸íŠ¸ëŠ” í•„ìš”í•  ë•Œë§Œ ìƒì„±\n",
                "- **ì´ë²¤íŠ¸ ë£¨í”„ ì•ˆì „**: ì´ˆê¸°í™” ì‹œì ì— Async í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì•ˆ í•¨\n",
                "- **ì—°ê²° ê´€ë¦¬**: ê²€ìƒ‰ í›„ `client.close()`ë¡œ ëª…ì‹œì  í•´ì œ\n",
                "\n",
                "---\n",
                "\n",
                "#### 3) `SparseEmbeddingManager`\n",
                "\n",
                "```python\n",
                "class SparseEmbeddingManager:\n",
                "    def initialize(self):\n",
                "        # BGE-M3 ëª¨ë¸ ë¡œë”©\n",
                "        self.model = BGEM3FlagModel(\n",
                "            self.config.SPARSE_EMBEDDING_MODEL,\n",
                "            use_fp16=torch.cuda.is_available()\n",
                "        )\n",
                "    \n",
                "    def encode_query(self, query: str) -> models.SparseVector:\n",
                "        # Sparse ë²¡í„° ìƒì„± (í‚¤ì›Œë“œ ë§¤ì¹­ìš©)\n",
                "        output = self.model.encode(\n",
                "            query,\n",
                "            return_sparse=True,\n",
                "            return_dense=False\n",
                "        )\n",
                "        weights = output['lexical_weights']  # {token_id: weight}\n",
                "        return models.SparseVector(indices=..., values=...)\n",
                "```\n",
                "\n",
                "**ì—­í• **: ë²•ë¥  ìš©ì–´ì˜ ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ì„ ìœ„í•œ í¬ì†Œ ë²¡í„° ìƒì„±\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.6 `graph.py`\n",
                "\n",
                "**ì—­í• **: **LangGraph ë…¸ë“œ ë° ì›Œí¬í”Œë¡œìš° êµ¬ì„±**\n",
                "\n",
                "#### í´ë˜ìŠ¤: `LegalRAGBuilder`\n",
                "\n",
                "```python\n",
                "class LegalRAGBuilder:\n",
                "    def __init__(self, config: Config):\n",
                "        self.config = config\n",
                "        self.llm = None\n",
                "        self.vs_manager = None\n",
                "        self.reranker = None\n",
                "    \n",
                "    def set_components(self, vs_manager, reranker):\n",
                "        # ë¯¸ë¦¬ ë¡œë”©ëœ ì»´í¬ë„ŒíŠ¸ ì£¼ì… (ì˜ì¡´ì„± ì£¼ì…)\n",
                "        self.vs_manager = vs_manager\n",
                "        self.reranker = reranker\n",
                "    \n",
                "    def build(self) -> CompiledStateGraph:\n",
                "        # ê·¸ë˜í”„ ë¹Œë“œ\n",
                "        builder = StateGraph(AgentState)\n",
                "        builder.add_node(\"analyze\", self._create_analyze_node())\n",
                "        builder.add_node(\"search\", self._create_search_node())\n",
                "        ...\n",
                "        return builder.compile()\n",
                "```\n",
                "\n",
                "#### ë…¸ë“œ ìƒì„± ë©”ì„œë“œ\n",
                "\n",
                "| ë©”ì„œë“œ | ë…¸ë“œëª… | ì£¼ìš” ë¡œì§ |\n",
                "|--------|--------|----------|\n",
                "| `_create_analyze_node()` | analyze | LLMìœ¼ë¡œ ì§ˆë¬¸ ë¶„ì„ â†’ `QueryAnalysis` ë°˜í™˜ |\n",
                "| `_create_clarify_node()` | clarify | ëª…í™•í™” ìš”ì²­ ë©”ì‹œì§€ ìƒì„± |\n",
                "| `_create_search_node()` | search | **í•µì‹¬**: Hybrid Search + Reranking |\n",
                "| `_create_generate_node()` | generate | ê²€ìƒ‰ ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ ìƒì„± |\n",
                "| `_create_evaluate_node()` | evaluate | ë‹µë³€ í’ˆì§ˆ í‰ê°€ |\n",
                "\n",
                "---\n",
                "\n",
                "#### í•µì‹¬ ë…¸ë“œ: `_create_search_node()` (L185-L296)\n",
                "\n",
                "```python\n",
                "async def search_documents(state: AgentState) -> dict:\n",
                "    # 0. Qdrant í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
                "    client = await self.vs_manager.create_client()\n",
                "    \n",
                "    try:\n",
                "        # 1. Query Expansion (HyDE)\n",
                "        hybrid = await query_expander(original_query)\n",
                "        keyword_query = hybrid.keyword_query\n",
                "        vector_query = hybrid.hyde_passage\n",
                "        \n",
                "        # 2. ì„ë² ë”© ìƒì„± (ë³‘ë ¬)\n",
                "        dense_vec, sparse_vec = await asyncio.gather(\n",
                "            asyncio.to_thread(embeddings.embed_query, vector_query),\n",
                "            asyncio.to_thread(sparse_manager.encode_query, keyword_query)\n",
                "        )\n",
                "        \n",
                "        # 3. Qdrant Hybrid Search\n",
                "        vector_docs = await self._execute_search(\n",
                "            client, dense_vec, sparse_vec, collection_name, limit=10\n",
                "        )\n",
                "        \n",
                "        # 4. Reranking\n",
                "        reranked_docs = await asyncio.to_thread(\n",
                "            reranker.compress_documents, vector_docs, original_query\n",
                "        )\n",
                "        \n",
                "        # 5. Filtering & Boosting\n",
                "        final_docs = [doc for doc in reranked_docs \n",
                "                      if doc.metadata['relevance_score'] >= 0.2][:3]\n",
                "    \n",
                "    finally:\n",
                "        await client.close()\n",
                "    \n",
                "    return {\"retrieved_docs\": final_docs}\n",
                "```\n",
                "\n",
                "**ì„±ëŠ¥ ìµœì í™” í¬ì¸íŠ¸**:\n",
                "- **ë³‘ë ¬ ì„ë² ë”©** (L229): Dense/Sparse ë™ì‹œ ìƒì„±\n",
                "- **ë¹„ë™ê¸° ê²€ìƒ‰** (L232): I/O ëŒ€ê¸° ì‹œê°„ ìµœì†Œí™”\n",
                "- **ì—°ê²° í•´ì œ** (L250): ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€\n",
                "\n",
                "---\n",
                "\n",
                "#### ë¼ìš°íŒ… ë¡œì§\n",
                "\n",
                "```python\n",
                "def _route_after_analysis(state) -> Literal[\"clarify\", \"search\"]:\n",
                "    if state[\"query_analysis\"][\"needs_clarification\"]:\n",
                "        return \"clarify\"\n",
                "    return \"search\"\n",
                "\n",
                "def _route_after_generate(state) -> Literal[\"evaluate\", \"end\"]:\n",
                "    complexity = state[\"query_analysis\"][\"query_complexity\"]\n",
                "    if complexity == \"simple\":\n",
                "        return \"end\"  # í‰ê°€ ìƒëµ\n",
                "    return \"evaluate\"\n",
                "\n",
                "def _route_after_evaluation(state) -> Literal[\"search\", \"end\"]:\n",
                "    if state[\"retry_count\"] >= MAX_RETRY:\n",
                "        return \"end\"\n",
                "    if state[\"evaluation_result\"][\"quality_score\"] <= 2:\n",
                "        return \"search\"  # ì¬ê²€ìƒ‰\n",
                "    return \"end\"\n",
                "```\n",
                "\n",
                "**ì¡°ê±´ë¶€ ì‹¤í–‰**ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ ë…¸ë“œ ì‹¤í–‰ ë°©ì§€ â†’ ì„±ëŠ¥ í–¥ìƒ\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. íŒŒì¼ ê°„ ê´€ê³„ë„\n",
                "\n",
                "### ì˜ì¡´ì„± ê·¸ë˜í”„\n",
                "\n",
                "```\n",
                "config.py (ì„¤ì •)\n",
                "    â†“\n",
                "prompts.py (í”„ë¡¬í”„íŠ¸)\n",
                "    â†“\n",
                "schemas.py (ë°ì´í„° êµ¬ì¡°)\n",
                "    â†“\n",
                "infrastructure.py (ì¸í”„ë¼)\n",
                "    â†“ (Config, Schemas ì‚¬ìš©)\n",
                "graph.py (ë¡œì§)\n",
                "    â†“ (ëª¨ë“  ëª¨ë“ˆ ì‚¬ìš©)\n",
                "__init__.py (ì§„ì…ì )\n",
                "```\n",
                "\n",
                "### ì„í¬íŠ¸ ê´€ê³„\n",
                "\n",
                "| íŒŒì¼ | ì„í¬íŠ¸í•˜ëŠ” ëª¨ë“ˆ |\n",
                "|------|------------------|\n",
                "| `config.py` | `django.conf.settings`, `os` |\n",
                "| `prompts.py` | (ì—†ìŒ - ìˆœìˆ˜ ë¬¸ìì—´) |\n",
                "| `schemas.py` | `pydantic`, `langchain_core` |\n",
                "| `infrastructure.py` | `config`, `torch`, `langchain_*` |\n",
                "| `graph.py` | `config`, `schemas`, `infrastructure`, `prompts` |\n",
                "| `__init__.py` | `config`, `infrastructure`, `graph` |\n",
                "\n",
                "### ë°ì´í„° íë¦„\n",
                "\n",
                "```\n",
                "ì‚¬ìš©ì ì§ˆë¬¸\n",
                "    â†“\n",
                "[graph.py] LegalRAGBuilder.build()\n",
                "    â†“\n",
                "[graph.py] analyze_node\n",
                "    â†’ [prompts.py] PROMPT_ANALYZE\n",
                "    â†’ [schemas.py] QueryAnalysis\n",
                "    â†“\n",
                "[graph.py] search_node\n",
                "    â†’ [infrastructure.py] VectorStoreManager\n",
                "    â†’ [infrastructure.py] SparseEmbeddingManager\n",
                "    â†’ [infrastructure.py] JinaReranker\n",
                "    â†“\n",
                "[graph.py] generate_node\n",
                "    â†’ [prompts.py] PROMPT_GENERATE\n",
                "    â†“\n",
                "ìµœì¢… ë‹µë³€\n",
                "```\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. ì„±ëŠ¥ ìµœì í™” ë°©ì•ˆ\n",
                "\n",
                "### í˜„ì¬ ë³‘ëª© ì§€ì  ë¶„ì„\n",
                "\n",
                "| ë‹¨ê³„ | ì†Œìš” ì‹œê°„ (ì˜ˆìƒ) | ë³‘ëª© ì›ì¸ |\n",
                "|------|------------------|----------|\n",
                "| **1. Analyze** | ~1ì´ˆ | LLM API í˜¸ì¶œ |\n",
                "| **2. Query Expansion** | ~1ì´ˆ | LLM API í˜¸ì¶œ |\n",
                "| **3. ì„ë² ë”© ìƒì„±** | ~0.5ì´ˆ | ëª¨ë¸ ì¶”ë¡  (CPU/GPU) |\n",
                "| **4. Qdrant ê²€ìƒ‰** | ~0.3ì´ˆ | ë„¤íŠ¸ì›Œí¬ I/O |\n",
                "| **5. Reranking** | ~0.5ì´ˆ | ëª¨ë¸ ì¶”ë¡  |\n",
                "| **6. Generate** | ~2ì´ˆ | LLM API í˜¸ì¶œ (ê¸´ í”„ë¡¬í”„íŠ¸) |\n",
                "| **7. Evaluate** | ~1ì´ˆ | LLM API í˜¸ì¶œ |\n",
                "| **ì´í•©** | **~6.3ì´ˆ** | |\n",
                "\n",
                "---\n",
                "\n",
                "### ìµœì í™” ì „ëµ\n",
                "\n",
                "#### ğŸš€ ì „ëµ 1: LLM í˜¸ì¶œ ìµœì†Œí™”\n",
                "\n",
                "**í˜„ì¬ ë¬¸ì œ**: ì´ 4ë²ˆì˜ LLM í˜¸ì¶œ (Analyze, Expand, Generate, Evaluate)\n",
                "\n",
                "**í•´ê²°ì±…**:\n",
                "\n",
                "1. **Simple ì§ˆë¬¸ Fast Path**\n",
                "   - Analyzeì—ì„œ `complexity == \"simple\"` íŒë‹¨ ì‹œ Expand, Evaluate ìƒëµ\n",
                "   - **ì ˆê°**: ~2ì´ˆ\n",
                "\n",
                "2. **Query Expansion ìºì‹±**\n",
                "   - ë™ì¼ ì§ˆë¬¸ íŒ¨í„´ì€ ìºì‹œì—ì„œ ì¬ì‚¬ìš©\n",
                "   - **ì ˆê°**: ~1ì´ˆ (ì¬ë°©ë¬¸ ì‹œ)\n",
                "\n",
                "3. **Evaluate ì¡°ê±´ë¶€ ì‹¤í–‰** (ì´ë¯¸ êµ¬í˜„ë¨)\n",
                "   - Simple ì§ˆë¬¸ì€ í‰ê°€ ìƒëµ\n",
                "\n",
                "---\n",
                "\n",
                "#### âš¡ ì „ëµ 2: ë³‘ë ¬ ì²˜ë¦¬ ê°•í™”\n",
                "\n",
                "**í˜„ì¬ ë¬¸ì œ**: ì¼ë¶€ ë‹¨ê³„ê°€ ìˆœì°¨ ì‹¤í–‰\n",
                "\n",
                "**í•´ê²°ì±…**:\n",
                "\n",
                "1. **Analyze + Query Expansion ë³‘ë ¬í™”**\n",
                "   ```python\n",
                "   analysis, hybrid = await asyncio.gather(\n",
                "       analyze_chain.ainvoke(query),\n",
                "       expand_chain.ainvoke(query)\n",
                "   )\n",
                "   ```\n",
                "   - **ì ˆê°**: ~1ì´ˆ\n",
                "\n",
                "2. **ì„ë² ë”© ë³‘ë ¬í™”** (ì´ë¯¸ êµ¬í˜„ë¨)\n",
                "   - Dense/Sparse ë™ì‹œ ìƒì„±\n",
                "\n",
                "---\n",
                "\n",
                "#### ğŸ”§ ì „ëµ 3: ëª¨ë¸ ìµœì í™”\n",
                "\n",
                "**í˜„ì¬ ë¬¸ì œ**: ì„ë² ë”©/Reranking ëª¨ë¸ì´ ë¬´ê±°ì›€\n",
                "\n",
                "**í•´ê²°ì±…**:\n",
                "\n",
                "1. **ì„ë² ë”© ëª¨ë¸ ê²½ëŸ‰í™”**\n",
                "   - `Qwen3-Embedding-0.6B` â†’ `all-MiniLM-L6-v2` (ë” ì‘ìŒ)\n",
                "   - **ì ˆê°**: ~0.2ì´ˆ\n",
                "\n",
                "2. **Reranker ì–‘ìí™”** (ì´ë¯¸ FP16 ì ìš©ë¨)\n",
                "   - INT8 ì–‘ìí™” ì¶”ê°€ ê³ ë ¤\n",
                "\n",
                "3. **ë°°ì¹˜ í¬ê¸° ì¡°ì •**\n",
                "   - Rerankerì˜ `max_length=512` â†’ `256`\n",
                "   - **ì ˆê°**: ~0.1ì´ˆ\n",
                "\n",
                "---\n",
                "\n",
                "#### ğŸ—„ï¸ ì „ëµ 4: ê²€ìƒ‰ ìµœì í™”\n",
                "\n",
                "**í˜„ì¬ ë¬¸ì œ**: ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ê°€ ë§ìŒ\n",
                "\n",
                "**í•´ê²°ì±…**:\n",
                "\n",
                "1. **TOP_K ê°’ ì¡°ì •**\n",
                "   - `TOP_K_VECTOR: 10 â†’ 7`\n",
                "   - `TOP_K_RERANK: 5 â†’ 3`\n",
                "   - **ì ˆê°**: ~0.2ì´ˆ\n",
                "\n",
                "2. **Qdrant íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•**\n",
                "   - `QDRANT_TIMEOUT: 10 â†’ 5`\n",
                "   - **íš¨ê³¼**: ì‹¤íŒ¨ ì‹œ ë¹ ë¥¸ í´ë°±\n",
                "\n",
                "3. **gRPC ì‚¬ìš©** (ì´ë¯¸ ì ìš©ë¨)\n",
                "   - `QDRANT_PREFER_GRPC: True`\n",
                "\n",
                "---\n",
                "\n",
                "#### ğŸ’¾ ì „ëµ 5: ìºì‹± ë„ì…\n",
                "\n",
                "**í•´ê²°ì±…**:\n",
                "\n",
                "1. **ì„ë² ë”© ìºì‹œ**\n",
                "   ```python\n",
                "   from functools import lru_cache\n",
                "   \n",
                "   @lru_cache(maxsize=100)\n",
                "   def get_embedding(text: str):\n",
                "       return embeddings.embed_query(text)\n",
                "   ```\n",
                "   - **ì ˆê°**: ~0.5ì´ˆ (ì¬ë°©ë¬¸ ì‹œ)\n",
                "\n",
                "2. **ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ**\n",
                "   - Redisì— ì§ˆë¬¸-ë¬¸ì„œ ë§¤í•‘ ì €ì¥\n",
                "   - **ì ˆê°**: ~1ì´ˆ (ì¬ë°©ë¬¸ ì‹œ)\n",
                "\n",
                "---\n",
                "\n",
                "### ìµœì í™” íš¨ê³¼ ìš”ì•½\n",
                "\n",
                "| ì „ëµ | ì˜ˆìƒ ì ˆê° ì‹œê°„ | ë‚œì´ë„ |\n",
                "|------|----------------|--------|\n",
                "| Simple Fast Path | ~2ì´ˆ | ì‰¬ì›€ |\n",
                "| Analyze + Expand ë³‘ë ¬í™” | ~1ì´ˆ | ì¤‘ê°„ |\n",
                "| TOP_K ê°’ ì¡°ì • | ~0.2ì´ˆ | ì‰¬ì›€ |\n",
                "| ì„ë² ë”© ìºì‹± | ~0.5ì´ˆ (ì¬ë°©ë¬¸) | ì¤‘ê°„ |\n",
                "| ê²€ìƒ‰ ê²°ê³¼ ìºì‹± | ~1ì´ˆ (ì¬ë°©ë¬¸) | ì–´ë ¤ì›€ |\n",
                "| **ì´ ì ˆê°** | **~4.7ì´ˆ** | |\n",
                "| **ìµœì¢… ì‘ë‹µ ì‹œê°„** | **~1.6ì´ˆ** | |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. ìµœì í™” ì½”ë“œ ìˆ˜ì • ê°€ì´ë“œ\n",
                "\n",
                "### ğŸ¯ ìµœì í™” 1: Simple Fast Path (ì¦‰ì‹œ ì ìš© ê°€ëŠ¥)\n",
                "\n",
                "**ìˆ˜ì • íŒŒì¼**: `graph.py`\n",
                "\n",
                "**ìˆ˜ì • ìœ„ì¹˜**: `_create_search_node()` ë©”ì„œë“œ ë‚´ë¶€ (L207-L219)\n",
                "\n",
                "**Before**:\n",
                "```python\n",
                "# 1. Query Expansion (Async)\n",
                "if query_expander:\n",
                "    hybrid = await query_expander(original_query)\n",
                "    keyword_query = hybrid.keyword_query\n",
                "    vector_query = hybrid.hyde_passage\n",
                "```\n",
                "\n",
                "**After**:\n",
                "```python\n",
                "# 1. Query Expansion (ì¡°ê±´ë¶€)\n",
                "complexity = analysis.get(\"query_complexity\", \"medium\")\n",
                "\n",
                "if complexity == \"simple\":\n",
                "    # Simple ì§ˆë¬¸ì€ Query Expansion ìƒëµ\n",
                "    keyword_query = original_query\n",
                "    vector_query = original_query\n",
                "    logger.info(\"Simple query - skipping query expansion\")\n",
                "elif query_expander:\n",
                "    hybrid = await query_expander(original_query)\n",
                "    keyword_query = hybrid.keyword_query\n",
                "    vector_query = hybrid.hyde_passage\n",
                "```\n",
                "\n",
                "**íš¨ê³¼**: Simple ì§ˆë¬¸ ì‹œ ~1ì´ˆ ì ˆê°\n",
                "\n",
                "**ì´ìœ **: \n",
                "- Simple ì§ˆë¬¸(ì˜ˆ: \"ê·¼ë¡œê¸°ì¤€ë²• ì œ2ì¡°ê°€ ë­ì•¼?\")ì€ ì´ë¯¸ ëª…í™•í•˜ë¯€ë¡œ Query Expansion ë¶ˆí•„ìš”\n",
                "- LLM API í˜¸ì¶œ 1íšŒ ì ˆì•½\n",
                "\n",
                "---\n",
                "\n",
                "### ğŸ¯ ìµœì í™” 2: TOP_K ê°’ ì¡°ì •\n",
                "\n",
                "**ìˆ˜ì • íŒŒì¼**: `config.py`\n",
                "\n",
                "**ìˆ˜ì • ìœ„ì¹˜**: L22-L24\n",
                "\n",
                "**Before**:\n",
                "```python\n",
                "TOP_K_VECTOR: int = getattr(settings, \"TOP_K_VECTOR\", 10)\n",
                "TOP_K_RERANK: int = getattr(settings, \"TOP_K_RERANK\", 5)\n",
                "TOP_K_FINAL: int = getattr(settings, \"TOP_K_FINAL\", 3)\n",
                "```\n",
                "\n",
                "**After**:\n",
                "```python\n",
                "TOP_K_VECTOR: int = getattr(settings, \"TOP_K_VECTOR\", 7)  # 10 â†’ 7\n",
                "TOP_K_RERANK: int = getattr(settings, \"TOP_K_RERANK\", 3)  # 5 â†’ 3\n",
                "TOP_K_FINAL: int = getattr(settings, \"TOP_K_FINAL\", 3)  # ìœ ì§€\n",
                "```\n",
                "\n",
                "**íš¨ê³¼**: ~0.2ì´ˆ ì ˆê°\n",
                "\n",
                "**ì´ìœ **:\n",
                "- ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ 10ê°œ â†’ 7ê°œ: Qdrant ê²€ìƒ‰ ì‹œê°„ ë‹¨ì¶•\n",
                "- Reranker ì²˜ë¦¬ ë¬¸ì„œ 5ê°œ â†’ 3ê°œ: Reranker ì¶”ë¡  ì‹œê°„ ë‹¨ì¶•\n",
                "- ìµœì¢… ë¬¸ì„œëŠ” 3ê°œë¡œ ìœ ì§€í•˜ì—¬ ë‹µë³€ í’ˆì§ˆ ë³´ì¥\n",
                "\n",
                "---\n",
                "\n",
                "### ğŸ¯ ìµœì í™” 3: Analyze + Expand ë³‘ë ¬í™”\n",
                "\n",
                "**ìˆ˜ì • íŒŒì¼**: `graph.py`\n",
                "\n",
                "**ìˆ˜ì • ìœ„ì¹˜**: ìƒˆë¡œìš´ ë…¸ë“œ ìƒì„± (L146 ì´í›„)\n",
                "\n",
                "**Before** (í˜„ì¬ êµ¬ì¡°):\n",
                "```\n",
                "analyze â†’ search (ë‚´ë¶€ì—ì„œ expand í˜¸ì¶œ)\n",
                "```\n",
                "\n",
                "**After** (ë³‘ë ¬ êµ¬ì¡°):\n",
                "```python\n",
                "def _create_parallel_analyze_expand_node(self):\n",
                "    \"\"\"Analyzeì™€ Query Expansionì„ ë³‘ë ¬ ì‹¤í–‰\"\"\"\n",
                "    analyze_llm = self.llm.with_structured_output(QueryAnalysis)\n",
                "    expand_llm = self.llm.with_structured_output(HybridQuery)\n",
                "    \n",
                "    async def parallel_node(state: AgentState) -> dict:\n",
                "        query = state[\"user_query\"]\n",
                "        \n",
                "        # ë³‘ë ¬ ì‹¤í–‰\n",
                "        analysis, hybrid = await asyncio.gather(\n",
                "            analyze_llm.ainvoke({\"query\": query}),\n",
                "            expand_llm.ainvoke({\"query\": query})\n",
                "        )\n",
                "        \n",
                "        return {\n",
                "            \"query_analysis\": analysis.model_dump(),\n",
                "            \"expanded_query\": hybrid.model_dump()\n",
                "        }\n",
                "    \n",
                "    return parallel_node\n",
                "```\n",
                "\n",
                "**ê·¸ë˜í”„ ìˆ˜ì •**:\n",
                "```python\n",
                "# build() ë©”ì„œë“œ ë‚´ë¶€\n",
                "builder.add_node(\"analyze_expand\", self._create_parallel_analyze_expand_node())\n",
                "builder.set_entry_point(\"analyze_expand\")\n",
                "```\n",
                "\n",
                "**íš¨ê³¼**: ~1ì´ˆ ì ˆê°\n",
                "\n",
                "**ì´ìœ **:\n",
                "- ë‘ LLM í˜¸ì¶œì´ ë™ì‹œì— ì‹¤í–‰ë¨\n",
                "- ë„¤íŠ¸ì›Œí¬ I/O ëŒ€ê¸° ì‹œê°„ ì¤‘ë³µ ì œê±°\n",
                "\n",
                "---\n",
                "\n",
                "### ğŸ¯ ìµœì í™” 4: ì„ë² ë”© ìºì‹±\n",
                "\n",
                "**ìˆ˜ì • íŒŒì¼**: `infrastructure.py`\n",
                "\n",
                "**ìˆ˜ì • ìœ„ì¹˜**: `VectorStoreManager` í´ë˜ìŠ¤ (L97-L155)\n",
                "\n",
                "**ì¶”ê°€ ì½”ë“œ**:\n",
                "```python\n",
                "from functools import lru_cache\n",
                "\n",
                "class VectorStoreManager:\n",
                "    def __init__(self, config: Config):\n",
                "        # ... ê¸°ì¡´ ì½”ë“œ ...\n",
                "        self._embedding_cache = {}  # ìºì‹œ ë”•ì…”ë„ˆë¦¬\n",
                "    \n",
                "    def get_embeddings(self) -> HuggingFaceEmbeddings:\n",
                "        # ... ê¸°ì¡´ ì½”ë“œ ...\n",
                "        \n",
                "        # ìºì‹± ë˜í¼ ì¶”ê°€\n",
                "        original_embed = self.embeddings.embed_query\n",
                "        \n",
                "        def cached_embed(text: str):\n",
                "            if text in self._embedding_cache:\n",
                "                logger.info(\"Embedding cache hit\")\n",
                "                return self._embedding_cache[text]\n",
                "            \n",
                "            result = original_embed(text)\n",
                "            self._embedding_cache[text] = result\n",
                "            return result\n",
                "        \n",
                "        self.embeddings.embed_query = cached_embed\n",
                "        return self.embeddings\n",
                "```\n",
                "\n",
                "**íš¨ê³¼**: ~0.5ì´ˆ ì ˆê° (ë™ì¼ ì§ˆë¬¸ ì¬ë°©ë¬¸ ì‹œ)\n",
                "\n",
                "**ì´ìœ **:\n",
                "- ë™ì¼í•œ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ì¬ì‚¬ìš©\n",
                "- ëª¨ë¸ ì¶”ë¡  ì‹œê°„ ì™„ì „ ì œê±°\n",
                "\n",
                "---\n",
                "\n",
                "### ğŸ¯ ìµœì í™” 5: Reranker ë°°ì¹˜ í¬ê¸° ì¡°ì •\n",
                "\n",
                "**ìˆ˜ì • íŒŒì¼**: `infrastructure.py`\n",
                "\n",
                "**ìˆ˜ì • ìœ„ì¹˜**: `JinaReranker.compress_documents()` (L68-L70)\n",
                "\n",
                "**Before**:\n",
                "```python\n",
                "inputs = self.tokenizer(\n",
                "    pairs, padding=True, truncation=True,\n",
                "    return_tensors=\"pt\", max_length=512\n",
                ")\n",
                "```\n",
                "\n",
                "**After**:\n",
                "```python\n",
                "inputs = self.tokenizer(\n",
                "    pairs, padding=True, truncation=True,\n",
                "    return_tensors=\"pt\", max_length=256  # 512 â†’ 256\n",
                ")\n",
                "```\n",
                "\n",
                "**íš¨ê³¼**: ~0.1ì´ˆ ì ˆê°\n",
                "\n",
                "**ì´ìœ **:\n",
                "- í† í° ê¸¸ì´ ë‹¨ì¶•ìœ¼ë¡œ Transformer ì—°ì‚°ëŸ‰ ê°ì†Œ\n",
                "- ë²•ë¥  ë¬¸ì„œëŠ” ë³´í†µ 256 í† í° ì´ë‚´ë¡œ í•µì‹¬ ë‚´ìš© í¬í•¨\n",
                "\n",
                "---\n",
                "\n",
                "### ğŸ“Š ìµœì í™” ì ìš© ìš°ì„ ìˆœìœ„\n",
                "\n",
                "| ìˆœìœ„ | ìµœì í™” | ë‚œì´ë„ | íš¨ê³¼ | ì¶”ì²œë„ |\n",
                "|------|--------|--------|------|--------|\n",
                "| 1 | TOP_K ê°’ ì¡°ì • | â­ | 0.2ì´ˆ | â­â­â­â­â­ |\n",
                "| 2 | Simple Fast Path | â­â­ | 1ì´ˆ | â­â­â­â­â­ |\n",
                "| 3 | Reranker ë°°ì¹˜ í¬ê¸° | â­ | 0.1ì´ˆ | â­â­â­â­ |\n",
                "| 4 | ì„ë² ë”© ìºì‹± | â­â­â­ | 0.5ì´ˆ | â­â­â­â­ |\n",
                "| 5 | Analyze + Expand ë³‘ë ¬í™” | â­â­â­â­ | 1ì´ˆ | â­â­â­ |\n",
                "\n",
                "**ê¶Œì¥ ì ìš© ìˆœì„œ**: 1 â†’ 2 â†’ 3 â†’ 4 â†’ 5\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ“ ê²°ë¡ \n",
                "\n",
                "### ëª¨ë“ˆí™”ì˜ ì¥ì \n",
                "\n",
                "1. **ìœ ì§€ë³´ìˆ˜ì„±**: ê° íŒŒì¼ì´ ëª…í™•í•œ ì—­í• ì„ ê°€ì§\n",
                "2. **í™•ì¥ì„±**: ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€ ì‹œ í•´ë‹¹ ëª¨ë“ˆë§Œ ìˆ˜ì •\n",
                "3. **í…ŒìŠ¤íŠ¸ ìš©ì´ì„±**: ê° ëª¨ë“ˆì„ ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥\n",
                "4. **ì¬ì‚¬ìš©ì„±**: ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì—ì„œë„ í™œìš© ê°€ëŠ¥\n",
                "\n",
                "### ì„±ëŠ¥ ìµœì í™” ìš”ì•½\n",
                "\n",
                "- **í˜„ì¬ ì‘ë‹µ ì‹œê°„**: ~6.3ì´ˆ\n",
                "- **ìµœì í™” í›„**: ~1.6ì´ˆ (ì•½ 75% ê°œì„ )\n",
                "- **í•µì‹¬ ì „ëµ**: LLM í˜¸ì¶œ ìµœì†Œí™”, ë³‘ë ¬ ì²˜ë¦¬, ìºì‹±\n",
                "\n",
                "### ë‹¤ìŒ ë‹¨ê³„\n",
                "\n",
                "1. **ëª¨ë‹ˆí„°ë§**: LangSmithë¡œ ê° ë…¸ë“œë³„ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •\n",
                "2. **A/B í…ŒìŠ¤íŠ¸**: ìµœì í™” ì „í›„ ë‹µë³€ í’ˆì§ˆ ë¹„êµ\n",
                "3. **í”„ë¡œë•ì…˜ ë°°í¬**: ì ì§„ì ìœ¼ë¡œ ìµœì í™” ì ìš©\n",
                "\n",
                "---\n",
                "\n",
                "**ì‘ì„± ì™„ë£Œ**: 2026-01-29  \n",
                "**ì´ íŒŒì¼ ìˆ˜**: 6ê°œ  \n",
                "**ì´ ì½”ë“œ ë¼ì¸**: 905ì¤„ (ê¸°ì¡´ 965ì¤„ì—ì„œ ë¦¬íŒ©í† ë§)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}